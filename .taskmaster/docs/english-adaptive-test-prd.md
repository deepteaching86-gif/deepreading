# ì˜ì–´ ëŠ¥ë™í˜• ë¬´í•™ë…„ì œ ì ì‘í˜• í…ŒìŠ¤íŠ¸ PRD (Product Requirements Document)

> **í”„ë¡œì íŠ¸**: ë¬¸í•´ë ¥ ì§„ë‹¨ í‰ê°€ ì‹œìŠ¤í…œ - ì˜ì–´ ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸ í…œí”Œë¦¿
> **ë²„ì „**: 1.0.0
> **ì‘ì„±ì¼**: 2025-01-20
> **ë¬¸ì„œ ìœ í˜•**: Product Requirements Document

---

## ğŸ“‹ ëª©ì°¨

1. [í”„ë¡œì íŠ¸ ê°œìš”](#í”„ë¡œì íŠ¸-ê°œìš”)
2. [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
3. [ê¸°ìˆ  ìŠ¤íƒ](#ê¸°ìˆ -ìŠ¤íƒ)
4. [ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„](#ë°ì´í„°ë² ì´ìŠ¤-ì„¤ê³„)
5. [í•µì‹¬ ê¸°ëŠ¥ ëª…ì„¸](#í•µì‹¬-ê¸°ëŠ¥-ëª…ì„¸)
6. [UI/UX ì„¤ê³„](#uiux-ì„¤ê³„)
7. [ê°œë°œ ë¡œë“œë§µ](#ê°œë°œ-ë¡œë“œë§µ)
8. [ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­](#ì„±ëŠ¥-ìš”êµ¬ì‚¬í•­)
9. [ë³´ì•ˆ ë° ê°œì¸ì •ë³´ë³´í˜¸](#ë³´ì•ˆ-ë°-ê°œì¸ì •ë³´ë³´í˜¸)
10. [í…ŒìŠ¤íŠ¸ ê³„íš](#í…ŒìŠ¤íŠ¸-ê³„íš)

---

## í”„ë¡œì íŠ¸ ê°œìš”

### 1.1 ë¹„ì „ (Vision)

**"í•™ë…„ì— êµ¬ì• ë°›ì§€ ì•ŠëŠ” ì •í™•í•œ ì˜ì–´ ëŠ¥ë ¥ ì§„ë‹¨ìœ¼ë¡œ ëª¨ë“  í•™ìƒì—ê²Œ ë§ì¶¤í˜• í•™ìŠµ ê²½ë¡œë¥¼ ì œê³µí•œë‹¤"**

### 1.2 ëª©í‘œ (Goals)

| ëª©í‘œ | ì¸¡ì • ì§€í‘œ | ëª©í‘œì¹˜ |
|------|----------|--------|
| **ì •í™•í•œ ëŠ¥ë ¥ ì¸¡ì •** | IRT SE(Standard Error) | â‰¤ 0.30 (95% í•™ìƒ) |
| **íš¨ìœ¨ì ì¸ í‰ê°€** | í‰ê°€ ì‹œê°„ | 20-30ë¶„ (40ë¬¸í•­) |
| **ë¬´í•™ë…„ì œ êµ¬í˜„** | 10ë ˆë²¨ ë²”ìœ„ | K-ëŒ€í•™ì› (AR 0.5~12.9) |
| **ë†’ì€ ì‚¬ìš©ì ë§Œì¡±ë„** | NPS(Net Promoter Score) | â‰¥ 50 |
| **ë¬¸í•­ ë³´ì•ˆ ìœ ì§€** | ìµœëŒ€ ë…¸ì¶œë¥  | â‰¤ 30% per item |

### 1.3 í•µì‹¬ ê°€ì¹˜ ì œì•ˆ (Value Proposition)

**í•™ìƒì—ê²Œ**:
- âœ… **ê°œì¸ ë§ì¶¤í˜•**: ì‹¤ë ¥ì— ë”± ë§ëŠ” ë‚œì´ë„ë¡œ 40ë¬¸í•­ë§Œ í’€ë©´ ì •í™•í•œ ì§„ë‹¨
- âœ… **ì‹œê°„ ì ˆì•½**: 300ê°œ ë¬¸í•­ í’€ ì¤‘ ìë™ ì„ íƒìœ¼ë¡œ 20-30ë¶„ ì™„ë£Œ
- âœ… **ëª…í™•í•œ ëª©í‘œ**: 10ë ˆë²¨ ì ˆëŒ€ ê¸°ì¤€ + Lexile/AR ì§€ìˆ˜ë¡œ ëª©í‘œ ì„¤ì • ê°€ëŠ¥
- âœ… **ì„¸ë°€í•œ í”¼ë“œë°±**: ë¬¸ë²•/ì–´íœ˜/ë…í•´ ë„ë©”ì¸ë³„ ê°•ì•½ì  ë¶„ì„

**êµì‚¬/ë¶€ëª¨ì—ê²Œ**:
- âœ… **ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²°ê³¼**: IRT ê¸°ë°˜ ê³¼í•™ì  ì¸¡ì • (í‘œì¤€ì˜¤ì°¨ â‰¤ 0.30)
- âœ… **í•™ë…„ ë¬´ê´€ ë°°ì¹˜**: ì‹¤ë ¥ë§Œìœ¼ë¡œ ë ˆë²¨ íŒë‹¨, ì„ ì…ê²¬ ë°°ì œ
- âœ… **êµ­ì œ í‘œì¤€ ì—°ë™**: Lexile Framework, AR ë“±ê¸‰ê³¼ í˜¸í™˜
- âœ… **êµ¬ì²´ì  ì–´íœ˜ëŸ‰**: "ì•½ 5,000ë‹¨ì–´ ìˆ˜ì¤€" ê°™ì€ ëª…í™•í•œ ì •ë³´

**ìš´ì˜ìì—ê²Œ**:
- âœ… **ë¬¸í•­ íš¨ìœ¨ì„±**: 300ê°œë¡œ ì‹œì‘ â†’ 600ê°œë¡œ í™•ì¥ ê°€ëŠ¥í•œ ë¡œë“œë§µ
- âœ… **ë³´ì•ˆ ìš°ìˆ˜**: MST íŒ¨ë„ íšŒì „ + Randomesqueë¡œ ë¬¸í•­ ë…¸ì¶œ ìµœì†Œí™”
- âœ… **ìœ ì§€ë³´ìˆ˜ ìš©ì´**: ë¶„ê¸°ë³„ IRT ì¬ìº˜ë¦¬ë¸Œë ˆì´ì…˜ìœ¼ë¡œ í’ˆì§ˆ ìœ ì§€

### 1.4 ë²”ìœ„ (Scope)

#### âœ… **í¬í•¨ ì‚¬í•­ (In Scope)**

**Phase 1 (MVP - 3ê°œì›”)**:
- [x] MST(Multistage Testing) 1â†’3â†’3 êµ¬ì¡° êµ¬í˜„
- [x] ë¬¸í•­ í’€ 300ê°œ ì œì‘ (ë¬¸ë²• 100, ì–´íœ˜ 100, ë…í•´ 100)
- [x] IRT 3PL ëª¨ë¸ ê¸°ë°˜ ëŠ¥ë ¥ì¹˜(Î¸) ì¶”ì •
- [x] 10ë ˆë²¨ ì ˆëŒ€ ê¸°ì¤€ ì§„ë‹¨
- [x] Lexile/AR ì¶”ì • (ML ëª¨ë¸)
- [x] ì–´íœ˜ ì‚¬ì´ì¦ˆ ì¶”ì • (VST + ê°€ì§œì–´)
- [x] ë„ë©”ì¸ë³„ ê°•ì•½ì  ë¶„ì„ (ë¬¸ë²•/ì–´íœ˜/ë…í•´)
- [x] í•™ìƒìš© ìƒì„¸ ë¦¬í¬íŠ¸ UI

**Phase 2 (í™•ì¥ - 6ê°œì›”)**:
- [ ] ë¬¸í•­ í’€ 600ê°œë¡œ í™•ì¥ (ë™í˜• 6ì„¸íŠ¸)
- [ ] ëª¨ë°”ì¼ ìµœì í™” (ë°˜ì‘í˜• ë””ìì¸)
- [ ] í•™ê¸‰/í•™êµ ëŒ€ì‹œë³´ë“œ (êµì‚¬ìš©)
- [ ] í•™ìŠµ ì¶”ì²œ ì‹œìŠ¤í…œ (ë ˆë²¨ë³„ ë„ì„œ/ì½˜í…ì¸ )

#### âŒ **ì œì™¸ ì‚¬í•­ (Out of Scope)**

- âŒ ë§í•˜ê¸°/ì“°ê¸° í‰ê°€ (ì£¼ê´€ì‹ ì±„ì  í•„ìš”)
- âŒ ì‹¤ì‹œê°„ í˜‘ì—… ê¸°ëŠ¥
- âŒ AI íŠœí„°ë§ (ë³„ë„ í”„ë¡œì íŠ¸)
- âŒ ë‹¤êµ­ì–´ ì§€ì› (ì˜ì–´ ì „ìš©)

### 1.5 ì‚¬ìš©ì í˜ë¥´ì†Œë‚˜ (User Personas)

#### **í˜ë¥´ì†Œë‚˜ 1: ì´ˆë“± 5í•™ë…„ ë¯¼ì§€** ğŸ‘§
- **ë°°ê²½**: í•™ì›ì—ì„œ ì¤‘1 ê³¼ì • ì„ í–‰í•™ìŠµ ì¤‘, ìê¸° ì‹¤ë ¥ì´ ê¶ê¸ˆí•¨
- **ëª©í‘œ**: ê°ê´€ì  ë ˆë²¨ í™•ì¸ â†’ ì ì ˆí•œ ì›ì„œ ì„ íƒ
- **Pain Point**: í•™ë…„ ê¸°ì¤€ í…ŒìŠ¤íŠ¸ëŠ” ë„ˆë¬´ ì‰¬ì›Œì„œ ë³€ë³„ë ¥ ì—†ìŒ
- **ì„±ê³µ ì‹œë‚˜ë¦¬ì˜¤**:
  - í…ŒìŠ¤íŠ¸ ê²°ê³¼ "Level 7 (AR 4.5~5.0, Lexile 850L)"
  - ì¶”ì²œ ë„ì„œ: *Charlotte's Web*, *Harry Potter 1* ë“±

#### **í˜ë¥´ì†Œë‚˜ 2: ì¤‘2 ì˜ì–´ êµì‚¬ ê¹€ ì„ ìƒë‹˜** ğŸ‘¨â€ğŸ«
- **ë°°ê²½**: í•™ê¸‰ 30ëª…ì˜ ì‹¤ë ¥ í¸ì°¨ ì‹¬í•¨ (ê¸°ì´ˆ~ê³ ê¸‰)
- **ëª©í‘œ**: í•™ìƒë³„ ì •í™•í•œ ë ˆë²¨ íŒŒì•… â†’ ìˆ˜ì¤€ë³„ ìˆ˜ì—… í¸ì„±
- **Pain Point**: í•™ë…„ë³„ ì§€í•„ê³ ì‚¬ëŠ” ì¤‘ìœ„ê¶Œ í•™ìƒ ë³€ë³„ë§Œ ê°€ëŠ¥
- **ì„±ê³µ ì‹œë‚˜ë¦¬ì˜¤**:
  - 30ëª… ì „ì› í…ŒìŠ¤íŠ¸ ì™„ë£Œ (ê° 25ë¶„ ì†Œìš”)
  - ë ˆë²¨ ë¶„í¬: Level 3 (2ëª…), Level 4 (5ëª…), ... Level 8 (3ëª…)
  - 3ê°œ ê·¸ë£¹ìœ¼ë¡œ ìˆ˜ì¤€ë³„ ìˆ˜ì—… í¸ì„±

#### **í˜ë¥´ì†Œë‚˜ 3: êµ­ì œí•™êµ ì „í•™ ì¤€ë¹„ ì¤‘ì¸ ê³ 1 ì§€í›ˆ** ğŸ“
- **ë°°ê²½**: ë¯¸êµ­ ê³ ë“±í•™êµ ì§€ì› ìœ„í•´ ì˜ì–´ ì‹¤ë ¥ ì¦ëª… í•„ìš”
- **ëª©í‘œ**: Lexile ì ìˆ˜ í™•ì¸ â†’ SAT/TOEFL ì¤€ë¹„ ì „ëµ ìˆ˜ë¦½
- **Pain Point**: í•™êµ ë‚´ì‹ ì€ Aì§€ë§Œ êµ­ì œ í‘œì¤€ ì ìˆ˜ ëª¨ë¦„
- **ì„±ê³µ ì‹œë‚˜ë¦¬ì˜¤**:
  - ì§„ë‹¨ ê²°ê³¼ "Lexile 1050L (Grade 7-8 ìˆ˜ì¤€)"
  - Gap ë¶„ì„: ë¬¸ë²•(ìƒ), ì–´íœ˜(ì¤‘), ë…í•´(ì¤‘ìƒ)
  - ëª©í‘œ: 6ê°œì›” ë‚´ 1200L ë‹¬ì„± â†’ SAT ì¤€ë¹„ ì‹œì‘

---

## ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### 2.1 High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Frontend (Next.js)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ Test UI    â”‚  â”‚ Report UI  â”‚  â”‚ Dashboard  â”‚             â”‚
â”‚  â”‚ (í•™ìƒìš©)    â”‚  â”‚ (ê²°ê³¼)      â”‚  â”‚ (êµì‚¬ìš©)    â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚ REST API
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Backend API (Python FastAPI)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ MST Engine                                               â”‚ â”‚
â”‚  â”‚  â”œâ”€ Stage 1 Router (8 items)                            â”‚ â”‚
â”‚  â”‚  â”œâ”€ Stage 2 Router (16 items, 3 tracks)                 â”‚ â”‚
â”‚  â”‚  â””â”€ Stage 3 Router (16 items, 9 subtracks)              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ IRT Engine (GIRTH)                                       â”‚ â”‚
â”‚  â”‚  â”œâ”€ EAP Î¸ Estimation                                    â”‚ â”‚
â”‚  â”‚  â”œâ”€ SE Calculation                                       â”‚ â”‚
â”‚  â”‚  â””â”€ 3PL Model (a, b, c parameters)                       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Diagnostic Engine                                        â”‚ â”‚
â”‚  â”‚  â”œâ”€ Lexile/AR ML Model (GradientBoosting)               â”‚ â”‚
â”‚  â”‚  â”œâ”€ Vocabulary Size Estimator (VST + Pseudo-words)      â”‚ â”‚
â”‚  â”‚  â””â”€ Domain Analysis (Grammar/Vocab/Reading)              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚ SQL
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Database (Supabase PostgreSQL)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ items    â”‚  â”‚ passages â”‚  â”‚ sessions â”‚  â”‚responses â”‚    â”‚
â”‚  â”‚ (ë¬¸í•­)    â”‚  â”‚ (ì§€ë¬¸)    â”‚  â”‚ (ì‘ì‹œ)    â”‚  â”‚ (ì‘ë‹µ)    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚
â”‚  â”‚mst_panelsâ”‚  â”‚students  â”‚                                 â”‚
â”‚  â”‚ (íŒ¨ë„)    â”‚  â”‚ (í•™ìƒ)    â”‚                                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 MST êµ¬ì¡° ìƒì„¸

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               STAGE 1: ë¼ìš°íŒ… ëª¨ë“ˆ (8ë¬¸í•­, 5ë¶„)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ëª©ì : í•™ìƒì„ Low/Med/High íŠ¸ë™ìœ¼ë¡œ ë¶„ë¥˜                    â”‚  â”‚
â”‚  â”‚ ë‚œì´ë„: Î¸ = -2.5 ~ +2.5 (ì „ ë²”ìœ„ ê³¨ê³ ë£¨)                   â”‚  â”‚
â”‚  â”‚ êµ¬ì„±: ë¬¸ë²• 3, ì–´íœ˜ 3, ë…í•´ 2                               â”‚  â”‚
â”‚  â”‚ ì¶”ì •: Î¸â‚ (SE â‰ˆ 0.8)                                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚               â”‚               â”‚
         â†“               â†“               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Low Track   â”‚  â”‚ Med Track   â”‚  â”‚ High Track  â”‚
â”‚ (16ë¬¸í•­)     â”‚  â”‚ (16ë¬¸í•­)     â”‚  â”‚ (16ë¬¸í•­)     â”‚
â”‚ Î¸: -2~0     â”‚  â”‚ Î¸: -1~+1    â”‚  â”‚ Î¸: 0~+2.5   â”‚
â”‚ êµ¬ì„±:        â”‚  â”‚ êµ¬ì„±:        â”‚  â”‚ êµ¬ì„±:        â”‚
â”‚ - ë¬¸ë²• 5    â”‚  â”‚ - ë¬¸ë²• 5    â”‚  â”‚ - ë¬¸ë²• 5    â”‚
â”‚ - ì–´íœ˜ 6    â”‚  â”‚ - ì–´íœ˜ 6    â”‚  â”‚ - ì–´íœ˜ 6    â”‚
â”‚ - ë…í•´ 5    â”‚  â”‚ - ë…í•´ 5    â”‚  â”‚ - ë…í•´ 5    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                â”‚                â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚              â”‚              â”‚
         â†“              â†“              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚Low-Low â”‚     â”‚Med-Med â”‚     â”‚High-   â”‚
    â”‚        â”‚ ... â”‚        â”‚ ... â”‚High    â”‚
    â”‚(16ë¬¸í•­)â”‚     â”‚(16ë¬¸í•­)â”‚     â”‚(16ë¬¸í•­)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“              â†“              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ìµœì¢… Î¸ ì¶”ì • (SE â‰¤ 0.30)               â”‚
    â”‚ â†’ 10ë ˆë²¨ ë³€í™˜                         â”‚
    â”‚ â†’ Lexile/AR ì¶”ì •                     â”‚
    â”‚ â†’ ì–´íœ˜ ì‚¬ì´ì¦ˆ ì¶”ì •                    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.3 ë°ì´í„° íë¦„ (Data Flow)

```
[í•™ìƒ]
  â†“ (1) í…ŒìŠ¤íŠ¸ ì‹œì‘
[Frontend: Test UI]
  â†“ (2) POST /api/test/start
[Backend: MST Engine]
  â†“ (3) Stage 1 ë¬¸í•­ 8ê°œ ë¡œë“œ
  â†“ (4) DBì—ì„œ ë¬¸í•­ ì¡°íšŒ (ë™í˜• ë¡œí…Œì´ì…˜)
[Database: items, mst_panels]
  â†“ (5) ë¬¸í•­ ë°ì´í„° ë°˜í™˜
[Backend: MST Engine]
  â†“ (6) JSON ì‘ë‹µ
[Frontend: Test UI]
  â†“ (7) í•™ìƒ ì‘ë‹µ (8ê°œ)
  â†“ (8) POST /api/test/submit-stage1
[Backend: IRT Engine]
  â†“ (9) EAP Î¸â‚ ì¶”ì •
  â†“ (10) ë¼ìš°íŒ… ê²°ì • (Low/Med/High)
  â†“ (11) Stage 2 ë¬¸í•­ 16ê°œ ë¡œë“œ (ì„ íƒëœ íŠ¸ë™)
[Database]
  â†“ (12) ë¬¸í•­ ë°˜í™˜
[Frontend]
  â†“ (13) í•™ìƒ ì‘ë‹µ (16ê°œ)
  â†“ (14) POST /api/test/submit-stage2
[Backend: IRT Engine]
  â†“ (15) EAP Î¸â‚‚ ì¶”ì •
  â†“ (16) ë¼ìš°íŒ… ê²°ì • (9ê°œ subtrack ì¤‘ 1ê°œ)
  â†“ (17) Stage 3 ë¬¸í•­ 16ê°œ ë¡œë“œ
[Database]
  â†“ (18) ë¬¸í•­ ë°˜í™˜
[Frontend]
  â†“ (19) í•™ìƒ ì‘ë‹µ (16ê°œ)
  â†“ (20) POST /api/test/complete
[Backend: Diagnostic Engine]
  â†“ (21) ìµœì¢… Î¸ ì¶”ì • (SE ê³„ì‚°)
  â†“ (22) Î¸ â†’ 10ë ˆë²¨ ë³€í™˜
  â†“ (23) Lexile/AR ML ëª¨ë¸ ì˜ˆì¸¡
  â†“ (24) ì–´íœ˜ ì‚¬ì´ì¦ˆ ì¶”ì • (VST)
  â†“ (25) ë„ë©”ì¸ë³„ ë¶„ì„
[Database: test_sessions, responses]
  â†“ (26) ê²°ê³¼ ì €ì¥
[Backend]
  â†“ (27) JSON ë¦¬í¬íŠ¸ ìƒì„±
[Frontend: Report UI]
  â†“ (28) ë¦¬í¬íŠ¸ ë Œë”ë§
[í•™ìƒ] (ì™„ë£Œ)
```

---

## ê¸°ìˆ  ìŠ¤íƒ

### 3.1 Frontend

| ê¸°ìˆ  | ë²„ì „ | ìš©ë„ | ë¼ì´ì„ ìŠ¤ |
|------|------|------|----------|
| **Next.js** | 14.x | React í”„ë ˆì„ì›Œí¬, SSR/SSG | MIT |
| **TypeScript** | 5.x | íƒ€ì… ì•ˆì •ì„± | Apache 2.0 |
| **Tailwind CSS** | 3.x | ìŠ¤íƒ€ì¼ë§ | MIT |
| **React Hook Form** | 7.x | í¼ ê´€ë¦¬ | MIT |
| **Recharts** | 2.x | ë¦¬í¬íŠ¸ ì°¨íŠ¸ (ë ˆì´ë”, ë§‰ëŒ€) | MIT |
| **Framer Motion** | 10.x | ì• ë‹ˆë©”ì´ì…˜ (ë¬¸í•­ ì „í™˜) | MIT |
| **Zustand** | 4.x | ìƒíƒœ ê´€ë¦¬ (í…ŒìŠ¤íŠ¸ ì§„í–‰ ìƒíƒœ) | MIT |

### 3.2 Backend (Python)

| ê¸°ìˆ  | ë²„ì „ | ìš©ë„ | ë¼ì´ì„ ìŠ¤ |
|------|------|------|----------|
| **FastAPI** | 0.109.x | REST API ì„œë²„ | MIT âœ… |
| **Uvicorn** | 0.27.x | ASGI ì„œë²„ | BSD-3 âœ… |
| **Pydantic** | 2.x | ë°ì´í„° ê²€ì¦ | MIT âœ… |
| **GIRTH** | 0.8.x | IRT íŒŒë¼ë¯¸í„° ì¶”ì • (3PL MML) | MIT âœ… |
| **NumPy** | 1.26.x | ìˆ˜ì¹˜ ê³„ì‚° | BSD âœ… |
| **SciPy** | 1.12.x | í†µê³„ í•¨ìˆ˜ (EAP, ì ë¶„) | BSD âœ… |
| **Pandas** | 2.2.x | ë°ì´í„° ì²˜ë¦¬ | BSD âœ… |
| **scikit-learn** | 1.4.x | ML ëª¨ë¸ (GradientBoosting) | BSD âœ… |
| **Textstat** | 0.7.x | ê°€ë…ì„± ì§€í‘œ (Flesch-Kincaid) | MIT âœ… |
| **Wordfreq** | 3.0.x | ì–´íœ˜ ë¹ˆë„ (Zipf scale) | Apache 2.0 + CC BY-SA 4.0 âœ… |
| **spaCy** | 3.7.x | NLP (ë¬¸ì¥ ë¶„ë¦¬, POS tagging) | MIT âœ… |

### 3.3 Database

| ê¸°ìˆ  | ìš©ë„ | ë¼ì´ì„ ìŠ¤ |
|------|------|----------|
| **Supabase** | PostgreSQL 15.x í˜¸ìŠ¤íŒ… | PostgreSQL (ììœ ) |
| **pgvector** | ì„ë² ë”© ì €ì¥ (í–¥í›„ í™•ì¥ìš©) | PostgreSQL âœ… |

### 3.4 DevOps & Deployment

| ê¸°ìˆ  | ìš©ë„ |
|------|------|
| **Netlify** | Frontend ë°°í¬ |
| **Render** | Backend ë°°í¬ (Python FastAPI) |
| **GitHub Actions** | CI/CD (ìë™ í…ŒìŠ¤íŠ¸, ë°°í¬) |
| **Docker** | ë°±ì—”ë“œ ì»¨í…Œì´ë„ˆí™” |

### 3.5 ê°œë°œ ë„êµ¬

| ë„êµ¬ | ìš©ë„ |
|------|------|
| **VSCode** | IDE |
| **Postman** | API í…ŒìŠ¤íŠ¸ |
| **DBeaver** | DB ê´€ë¦¬ |
| **Jupyter Notebook** | IRT ëª¨ë¸ ì‹¤í—˜, ë°ì´í„° ë¶„ì„ |

---

## ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„

### 4.1 ERD (Entity Relationship Diagram)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   students      â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ id (PK)         â”‚â”€â”€â”€â”
â”‚ email           â”‚   â”‚
â”‚ name            â”‚   â”‚
â”‚ grade_level     â”‚   â”‚
â”‚ school          â”‚   â”‚
â”‚ created_at      â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                      â”‚
                      â”‚ 1:N
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ test_sessions   â”‚   â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   â”‚
â”‚ id (PK)         â”‚   â”‚
â”‚ student_id (FK) â”‚â”€â”€â”€â”˜
â”‚ stage1_panel    â”‚
â”‚ stage2_panel    â”‚
â”‚ stage3_panel    â”‚
â”‚ route_path      â”‚
â”‚ theta_trace     â”‚ (JSONB)
â”‚ final_theta     â”‚
â”‚ final_se        â”‚
â”‚ diagnostic_levelâ”‚
â”‚ estimated_lexileâ”‚
â”‚ estimated_ar    â”‚
â”‚ vocabulary_size â”‚
â”‚ grammar_score   â”‚
â”‚ vocabulary_scoreâ”‚
â”‚ reading_score   â”‚
â”‚ started_at      â”‚
â”‚ completed_at    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 1:N
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   responses     â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ id (PK)         â”‚
â”‚ session_id (FK) â”‚â”€â”€â”€â”
â”‚ item_id (FK)    â”‚   â”‚
â”‚ stage           â”‚   â”‚
â”‚ sequence_num    â”‚   â”‚
â”‚ response        â”‚   â”‚
â”‚ is_correct      â”‚   â”‚
â”‚ response_time_msâ”‚   â”‚
â”‚ theta_at_time   â”‚   â”‚
â”‚ created_at      â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                      â”‚ N:1
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚     items       â”‚â”€â”€â”€â”˜
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ id (PK)         â”‚
â”‚ passage_id (FK) â”‚â”€â”€â”€â”
â”‚ stem            â”‚   â”‚
â”‚ options         â”‚ (JSONB)
â”‚ correct_answer  â”‚   â”‚
â”‚ discrimination  â”‚ (a)
â”‚ difficulty      â”‚ (b)
â”‚ guessing        â”‚ (c)
â”‚ domain          â”‚
â”‚ skill_tag       â”‚
â”‚ passage_type    â”‚
â”‚ stage           â”‚
â”‚ panel           â”‚
â”‚ form_id         â”‚
â”‚ slot_position   â”‚
â”‚ exposure_count  â”‚
â”‚ status          â”‚
â”‚ created_at      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                      â”‚ N:1
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   passages      â”‚â”€â”€â”€â”˜
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ id (PK)         â”‚
â”‚ text            â”‚
â”‚ word_count      â”‚
â”‚ mean_sentence_  â”‚
â”‚  length         â”‚
â”‚ mean_zipf       â”‚
â”‚ flesch_kincaid  â”‚
â”‚ estimated_lexileâ”‚
â”‚ estimated_ar    â”‚
â”‚ passage_type    â”‚
â”‚ topic           â”‚
â”‚ created_at      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  mst_panels     â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ id (PK)         â”‚
â”‚ panel_key       â”‚ (UNIQUE)
â”‚ stage           â”‚
â”‚ track           â”‚
â”‚ form_id         â”‚
â”‚ item_ids        â”‚ (INT[])
â”‚ grammar_count   â”‚
â”‚ vocabulary_countâ”‚
â”‚ reading_count   â”‚
â”‚ theta_min       â”‚
â”‚ theta_max       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 ìŠ¤í‚¤ë§ˆ ìƒì„¸ (PostgreSQL DDL)

```sql
-- í•™ìƒ í…Œì´ë¸”
CREATE TABLE students (
    id SERIAL PRIMARY KEY,
    email VARCHAR(255) UNIQUE NOT NULL,
    name VARCHAR(100) NOT NULL,
    grade_level INT CHECK (grade_level BETWEEN 1 AND 12),
    school VARCHAR(200),
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_students_email ON students(email);

-- ì§€ë¬¸ í…Œì´ë¸”
CREATE TABLE passages (
    id SERIAL PRIMARY KEY,
    text TEXT NOT NULL,
    word_count INT NOT NULL,

    -- í…ìŠ¤íŠ¸ íŠ¹ì§• (Lexile/AR ì¶”ì •ìš©)
    mean_sentence_length FLOAT,
    mean_word_length FLOAT,
    mean_zipf FLOAT,  -- Wordfreq Zipf scale (1-7)
    low_freq_ratio FLOAT,  -- ì €ë¹ˆë„ ë‹¨ì–´ ë¹„ìœ¨
    flesch_kincaid FLOAT,
    dale_chall FLOAT,
    subordinate_ratio FLOAT,  -- ì¢…ì†ì ˆ ë¹„ìœ¨

    -- ML ëª¨ë¸ ì¶”ì • ê²°ê³¼
    estimated_lexile INT,
    estimated_ar FLOAT,

    -- ë©”íƒ€ë°ì´í„°
    passage_type VARCHAR(20) CHECK (passage_type IN ('Expository', 'Argumentative', 'Narrative', 'Practical')),
    topic VARCHAR(50),

    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_passages_type ON passages(passage_type);
CREATE INDEX idx_passages_lexile ON passages(estimated_lexile);

-- ë¬¸í•­ í…Œì´ë¸”
CREATE TABLE items (
    id SERIAL PRIMARY KEY,
    passage_id INT REFERENCES passages(id) ON DELETE SET NULL,
    stem TEXT NOT NULL,
    options JSONB NOT NULL,  -- ["A) ...", "B) ...", "C) ...", "D) ..."]
    correct_answer CHAR(1) NOT NULL CHECK (correct_answer IN ('A', 'B', 'C', 'D')),

    -- IRT 3PL íŒŒë¼ë¯¸í„° (GIRTHë¡œ ì¶”ì •)
    discrimination FLOAT CHECK (discrimination > 0),  -- a (0.5 ~ 2.5)
    difficulty FLOAT CHECK (difficulty BETWEEN -3 AND 3),  -- b
    guessing FLOAT DEFAULT 0.25 CHECK (guessing BETWEEN 0 AND 0.5),  -- c

    -- ë©”íƒ€ë°ì´í„°
    domain VARCHAR(20) NOT NULL CHECK (domain IN ('Grammar', 'Vocabulary', 'Reading')),
    skill_tag VARCHAR(50),  -- 'Tenses', 'Synonyms', 'Main Idea' ë“±
    passage_type VARCHAR(20),  -- ë…í•´ ë¬¸í•­ë§Œ í•´ë‹¹

    -- MST ë°°ì¹˜
    stage INT NOT NULL CHECK (stage IN (1, 2, 3)),
    panel VARCHAR(30) NOT NULL,  -- 'stage1', 'stage2_Low', 'stage3_Med-High' ë“±
    form_id INT NOT NULL CHECK (form_id BETWEEN 0 AND 5),  -- ë™í˜• ì„¸íŠ¸ (0~5)
    slot_position INT NOT NULL CHECK (slot_position >= 1),  -- íŒ¨ë„ ë‚´ ìœ„ì¹˜

    -- ë…¸ì¶œ ì œì–´
    exposure_count INT DEFAULT 0,
    exposure_cap INT DEFAULT 1000,
    status VARCHAR(10) DEFAULT 'active' CHECK (status IN ('active', 'retired', 'flagged', 'pilot')),

    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_items_domain ON items(domain);
CREATE INDEX idx_items_stage_panel ON items(stage, panel);
CREATE INDEX idx_items_status ON items(status);
CREATE INDEX idx_items_exposure ON items(exposure_count);

-- MST íŒ¨ë„ ì •ì˜ í…Œì´ë¸”
CREATE TABLE mst_panels (
    id SERIAL PRIMARY KEY,
    panel_key VARCHAR(30) UNIQUE NOT NULL,  -- 'stage2_Low_form0'
    stage INT NOT NULL CHECK (stage IN (1, 2, 3)),
    track VARCHAR(10) CHECK (track IN ('Low', 'Med', 'High')),
    subtrack VARCHAR(20),  -- 'Low-Low', 'Low-Med', ... (Stage 3ë§Œ í•´ë‹¹)
    form_id INT NOT NULL CHECK (form_id BETWEEN 0 AND 5),

    -- íŒ¨ë„ ë‚´ ë¬¸í•­ ID ë°°ì—´
    item_ids INT[] NOT NULL,

    -- ì½˜í…ì¸  ê· í˜• ê²€ì¦
    grammar_count INT NOT NULL,
    vocabulary_count INT NOT NULL,
    reading_count INT NOT NULL,

    -- ë¼ìš°íŒ… ì¡°ê±´
    theta_min FLOAT,
    theta_max FLOAT,

    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_mst_panels_key ON mst_panels(panel_key);
CREATE INDEX idx_mst_panels_stage ON mst_panels(stage);

-- í…ŒìŠ¤íŠ¸ ì„¸ì…˜ í…Œì´ë¸”
CREATE TABLE test_sessions (
    id SERIAL PRIMARY KEY,
    student_id INT REFERENCES students(id) ON DELETE CASCADE,

    -- MST ê²½ë¡œ
    stage1_panel VARCHAR(30),
    stage2_panel VARCHAR(30),
    stage3_panel VARCHAR(30),
    route_path VARCHAR(100),  -- 'Low â†’ Low-Med'

    -- Î¸ ì¶”ì • ì´ë ¥
    theta_trace JSONB,  -- [{"stage": 1, "theta": 0.0, "se": 0.8}, ...]
    se_trace JSONB,

    -- ìµœì¢… ê²°ê³¼
    final_theta FLOAT,
    final_se FLOAT,

    -- ë¦¬í¬íŠ¸ ë°ì´í„°
    diagnostic_level INT CHECK (diagnostic_level BETWEEN 1 AND 10),
    estimated_lexile INT CHECK (estimated_lexile BETWEEN 0 AND 1700),
    estimated_ar FLOAT CHECK (estimated_ar BETWEEN 0 AND 12.9),
    vocabulary_size INT,
    vocab_confidence VARCHAR(10) CHECK (vocab_confidence IN ('High', 'Low')),

    -- ë„ë©”ì¸ë³„ ë¶„ì„ (0.0 ~ 1.0)
    grammar_score FLOAT CHECK (grammar_score BETWEEN 0 AND 1),
    vocabulary_score FLOAT CHECK (vocabulary_score BETWEEN 0 AND 1),
    reading_score FLOAT CHECK (reading_score BETWEEN 0 AND 1),

    -- íƒ€ì„ìŠ¤íƒ¬í”„
    started_at TIMESTAMP DEFAULT NOW(),
    completed_at TIMESTAMP,

    -- ì´ ì†Œìš” ì‹œê°„ (ì´ˆ)
    total_duration_sec INT,

    CONSTRAINT chk_completion CHECK (
        (completed_at IS NULL AND final_theta IS NULL) OR
        (completed_at IS NOT NULL AND final_theta IS NOT NULL)
    )
);

CREATE INDEX idx_test_sessions_student ON test_sessions(student_id);
CREATE INDEX idx_test_sessions_completed ON test_sessions(completed_at);
CREATE INDEX idx_test_sessions_level ON test_sessions(diagnostic_level);

-- ì‘ë‹µ ë¡œê·¸ í…Œì´ë¸”
CREATE TABLE responses (
    id SERIAL PRIMARY KEY,
    session_id INT REFERENCES test_sessions(id) ON DELETE CASCADE,
    item_id INT REFERENCES items(id) ON DELETE SET NULL,
    stage INT NOT NULL CHECK (stage IN (1, 2, 3)),
    sequence_num INT NOT NULL CHECK (sequence_num BETWEEN 1 AND 40),

    response CHAR(1) CHECK (response IN ('A', 'B', 'C', 'D')),
    is_correct BOOLEAN NOT NULL,
    response_time_ms INT CHECK (response_time_ms >= 0),

    -- ì¶”ì •ê°’ (í•´ë‹¹ ì‹œì )
    theta_at_time FLOAT,

    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_responses_session ON responses(session_id);
CREATE INDEX idx_responses_item ON responses(item_id);
CREATE INDEX idx_responses_created ON responses(created_at);

-- ë¬¸í•­ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì´ë ¥ í…Œì´ë¸”
CREATE TABLE calibration_runs (
    id SERIAL PRIMARY KEY,
    run_date TIMESTAMP DEFAULT NOW(),
    method VARCHAR(20) NOT NULL,  -- '3PL-MML', '2PL-MCMC', '3PL-MCMC'
    sample_size INT NOT NULL,

    -- ì í•©ë„ ì§€í‘œ
    log_likelihood FLOAT,
    aic FLOAT,  -- Akaike Information Criterion
    bic FLOAT,  -- Bayesian Information Criterion

    -- ë³€ê²½ì‚¬í•­
    items_updated INT DEFAULT 0,
    items_flagged INT DEFAULT 0,

    notes TEXT
);

CREATE INDEX idx_calibration_runs_date ON calibration_runs(run_date DESC);

-- ë·°: ë¬¸í•­ í†µê³„ ìš”ì•½
CREATE VIEW item_statistics AS
SELECT
    i.id,
    i.domain,
    i.stage,
    i.panel,
    i.exposure_count,
    i.status,
    COUNT(r.id) AS total_responses,
    AVG(CASE WHEN r.is_correct THEN 1 ELSE 0 END) AS p_value,  -- ì •ë‹µë¥ 
    CORR(
        CASE WHEN r.is_correct THEN 1 ELSE 0 END,
        ts.final_theta
    ) AS point_biserial  -- ë¬¸í•­-ì´ì  ìƒê´€
FROM items i
LEFT JOIN responses r ON i.id = r.item_id
LEFT JOIN test_sessions ts ON r.session_id = ts.id
WHERE ts.completed_at IS NOT NULL
GROUP BY i.id;

-- ë·°: í•™ìƒ í…ŒìŠ¤íŠ¸ ì´ë ¥
CREATE VIEW student_test_history AS
SELECT
    s.id AS student_id,
    s.name,
    s.email,
    ts.id AS session_id,
    ts.completed_at,
    ts.diagnostic_level,
    ts.estimated_lexile,
    ts.estimated_ar,
    ts.vocabulary_size,
    ts.final_theta,
    ts.final_se,
    ts.total_duration_sec / 60.0 AS duration_minutes
FROM students s
JOIN test_sessions ts ON s.id = ts.student_id
WHERE ts.completed_at IS NOT NULL
ORDER BY s.id, ts.completed_at DESC;
```

### 4.3 ìƒ˜í”Œ ë°ì´í„° êµ¬ì¡°

#### **items í…Œì´ë¸” ì˜ˆì‹œ**

```json
{
  "id": 42,
  "passage_id": 15,
  "stem": "Which word best completes the sentence?\n\n\"The scientist's discovery was _____, changing our understanding of the universe.\"",
  "options": [
    "A) revolutionary",
    "B) ordinary",
    "C) temporary",
    "D) invisible"
  ],
  "correct_answer": "A",
  "discrimination": 1.25,
  "difficulty": 0.8,
  "guessing": 0.25,
  "domain": "Vocabulary",
  "skill_tag": "Academic Adjectives",
  "passage_type": null,
  "stage": 2,
  "panel": "stage2_Med",
  "form_id": 0,
  "slot_position": 7,
  "exposure_count": 234,
  "status": "active"
}
```

#### **mst_panels í…Œì´ë¸” ì˜ˆì‹œ**

```json
{
  "id": 5,
  "panel_key": "stage2_Low_form0",
  "stage": 2,
  "track": "Low",
  "subtrack": null,
  "form_id": 0,
  "item_ids": [12, 15, 23, 28, 31, 35, 42, 47, 51, 58, 62, 65, 71, 78, 83, 89],
  "grammar_count": 5,
  "vocabulary_count": 6,
  "reading_count": 5,
  "theta_min": -2.0,
  "theta_max": 0.0
}
```

#### **test_sessions í…Œì´ë¸” ì˜ˆì‹œ**

```json
{
  "id": 1523,
  "student_id": 87,
  "stage1_panel": "stage1_form1",
  "stage2_panel": "stage2_Med_form2",
  "stage3_panel": "stage3_Med-High_form0",
  "route_path": "Med â†’ Med-High",
  "theta_trace": [
    {"stage": 1, "theta": 0.2, "se": 0.75},
    {"stage": 2, "theta": 0.5, "se": 0.45},
    {"stage": 3, "theta": 0.62, "se": 0.28}
  ],
  "final_theta": 0.62,
  "final_se": 0.28,
  "diagnostic_level": 7,
  "estimated_lexile": 950,
  "estimated_ar": 5.2,
  "vocabulary_size": 6200,
  "vocab_confidence": "High",
  "grammar_score": 0.75,
  "vocabulary_score": 0.68,
  "reading_score": 0.72,
  "started_at": "2025-01-20T10:30:00Z",
  "completed_at": "2025-01-20T10:55:32Z",
  "total_duration_sec": 1532
}
```

---

## í•µì‹¬ ê¸°ëŠ¥ ëª…ì„¸

### 5.1 MST Engine (Multistage Testing)

#### 5.1.1 Stage 1: ë¼ìš°íŒ… ëª¨ë“ˆ

**ìš”êµ¬ì‚¬í•­**:
- [x] 8ë¬¸í•­ ê³ ì • êµ¬ì„±
- [x] ë„ë©”ì¸ ê· í˜•: ë¬¸ë²• 3, ì–´íœ˜ 3, ë…í•´ 2
- [x] ë‚œì´ë„ ë²”ìœ„: Î¸ = -2.5 ~ +2.5 (ì „ì²´ ìŠ¤í™íŠ¸ëŸ¼)
- [x] ë™í˜• 3ì„¸íŠ¸ ì¤€ë¹„ (form 0, 1, 2)
- [x] EAP Î¸â‚ ì¶”ì • í›„ Low/Med/High íŠ¸ë™ ë¶„ë¥˜

**ë¶„ë¥˜ ê¸°ì¤€**:
```python
if theta1 < -0.5:
    track = 'Low'     # Î¸: -2.5 ~ -0.5
elif theta1 < 0.5:
    track = 'Med'     # Î¸: -0.5 ~ 0.5
else:
    track = 'High'    # Î¸: 0.5 ~ 2.5
```

**API ì—”ë“œí¬ì¸íŠ¸**:
```
POST /api/test/start
Request Body:
{
  "student_id": 87
}

Response:
{
  "session_id": 1523,
  "stage": 1,
  "items": [
    {
      "id": 5,
      "stem": "Choose the correct verb form...",
      "options": ["A) has", "B) have", "C) had", "D) having"],
      "domain": "Grammar",
      "passage": null
    },
    // ... 8ê°œ
  ]
}
```

#### 5.1.2 Stage 2: íŠ¸ë™ë³„ ì„¸ë¶€ í‰ê°€

**ìš”êµ¬ì‚¬í•­**:
- [x] 16ë¬¸í•­ ê³ ì • êµ¬ì„±
- [x] ë„ë©”ì¸ ê· í˜•: ë¬¸ë²• 5, ì–´íœ˜ 6, ë…í•´ 5
- [x] 3ê°œ íŠ¸ë™ ê°ê° 3ê°œ ë™í˜• (ì´ 9ê°œ íŒ¨ë„)
- [x] íŠ¸ë™ë³„ ë‚œì´ë„ ë²”ìœ„:
  - Low: Î¸ = -2.0 ~ 0.0
  - Med: Î¸ = -1.0 ~ +1.0
  - High: Î¸ = 0.0 ~ +2.5

**API ì—”ë“œí¬ì¸íŠ¸**:
```
POST /api/test/submit-stage1
Request Body:
{
  "session_id": 1523,
  "responses": [
    {"item_id": 5, "answer": "B", "response_time_ms": 12500},
    // ... 8ê°œ
  ]
}

Response:
{
  "theta1": 0.2,
  "se1": 0.75,
  "track": "Med",
  "stage": 2,
  "items": [
    // ... 16ê°œ (Med íŠ¸ë™)
  ]
}
```

#### 5.1.3 Stage 3: ì •ë°€ ì§„ë‹¨

**ìš”êµ¬ì‚¬í•­**:
- [x] 16ë¬¸í•­ ê³ ì • êµ¬ì„±
- [x] ë„ë©”ì¸ ê· í˜•: ë¬¸ë²• 5, ì–´íœ˜ 6, ë…í•´ 5
- [x] 9ê°œ ì„œë¸ŒíŠ¸ë™ (ê° íŠ¸ë™ì—ì„œ 3ê°ˆë˜ ë¶„ê¸°)
  - Low â†’ Low-Low, Low-Med, Low-High
  - Med â†’ Med-Low, Med-Med, Med-High
  - High â†’ High-Low, High-Med, High-High
- [x] ê° ì„œë¸ŒíŠ¸ë™ 3ê°œ ë™í˜• (ì´ 27ê°œ íŒ¨ë„)

**ë¶„ë¥˜ ê¸°ì¤€ (ì˜ˆ: Med íŠ¸ë™)**:
```python
if theta2 < -0.25:
    subtrack = 'Med-Low'
elif theta2 < 0.25:
    subtrack = 'Med-Med'
else:
    subtrack = 'Med-High'
```

**API ì—”ë“œí¬ì¸íŠ¸**:
```
POST /api/test/submit-stage2
Request Body:
{
  "session_id": 1523,
  "responses": [
    {"item_id": 42, "answer": "A", "response_time_ms": 18200},
    // ... 16ê°œ
  ]
}

Response:
{
  "theta2": 0.5,
  "se2": 0.45,
  "subtrack": "Med-High",
  "stage": 3,
  "items": [
    // ... 16ê°œ (Med-High ì„œë¸ŒíŠ¸ë™)
  ]
}
```

#### 5.1.4 Randomesque ë¬¸í•­ ì„ íƒ

**ìš”êµ¬ì‚¬í•­**:
- [x] ê° ìŠ¬ë¡¯ì— 2-3ê°œ ìœ ì‚¬ ë‚œì´ë„ ë¬¸í•­ í›„ë³´ ì¤€ë¹„
- [x] ì‹¤ì‹œê°„ ë¬´ì‘ìœ„ ì„ íƒìœ¼ë¡œ ë…¸ì¶œë¥  ë¶„ì‚°
- [x] ë™ì¼ í•™ìƒ ì¬ì‘ì‹œ ì‹œ ë‹¤ë¥¸ ë¬¸í•­ ì¡°í•©

**êµ¬í˜„ ì˜ˆì‹œ**:
```python
# mst_panels í…Œì´ë¸”ì—ì„œ íŒ¨ë„ ë¡œë“œ
panel = get_panel('stage2_Med_form0')

# item_ids = [42, 45, 47, ...]ëŠ” ê° ìŠ¬ë¡¯ì˜ ëŒ€í‘œ ë¬¸í•­
# ì‹¤ì œë¡œëŠ” ê° ìŠ¬ë¡¯ì— 2-3ê°œ í›„ë³´ ì €ì¥ (ë³„ë„ í…Œì´ë¸”)
selected_items = []
for slot in panel['slots']:
    candidates = slot['candidate_item_ids']  # [42, 43, 44]
    # ë‚œì´ë„ ì°¨ì´ Â± 0.2 ì´ë‚´
    chosen = random.choice(candidates)
    selected_items.append(chosen)

    # ë…¸ì¶œ ì¹´ìš´íŠ¸ ì¦ê°€
    increment_exposure(chosen)
```

### 5.2 IRT Engine (3PL EAP Estimation)

#### 5.2.1 EAP Î¸ ì¶”ì •

**ìš”êµ¬ì‚¬í•­**:
- [x] 3PL ëª¨ë¸ ì‚¬ìš© (a, b, c íŒŒë¼ë¯¸í„°)
- [x] EAP(Expected A Posteriori) ë°©ì‹ìœ¼ë¡œ Î¸ ì¶”ì •
- [x] Prior: N(0, 1) (í‘œì¤€ì •ê·œë¶„í¬)
- [x] Posterior: Bayes ì •ë¦¬ ì ìš©
- [x] SE(Standard Error) ê³„ì‚°

**3PL ëª¨ë¸ ê³µì‹**:
```
P(X_i = 1 | Î¸) = c_i + (1 - c_i) / (1 + exp(-a_i * (Î¸ - b_i)))

where:
  a_i: discrimination (ë³€ë³„ë„, 0.5 ~ 2.5)
  b_i: difficulty (ë‚œì´ë„, -3 ~ +3)
  c_i: guessing (ì¶”ì¸¡ë„, ë³´í†µ 0.25)
```

**EAP ì¶”ì • ê³µì‹**:
```
Î¸_EAP = E[Î¸ | X] = âˆ« Î¸ * L(X | Î¸) * Ï€(Î¸) dÎ¸ / âˆ« L(X | Î¸) * Ï€(Î¸) dÎ¸

where:
  L(X | Î¸): Likelihood (ë¬¸í•­ ì‘ë‹µ íŒ¨í„´ ê¸°ë°˜)
  Ï€(Î¸): Prior (N(0, 1))
```

**êµ¬í˜„ (Python)**:
```python
from scipy.stats import norm
from scipy.integrate import quad
import numpy as np

def three_pl_probability(theta, a, b, c):
    """3PL ëª¨ë¸ ì •ë‹µ í™•ë¥ """
    return c + (1 - c) / (1 + np.exp(-a * (theta - b)))

def eap_estimate(responses, items, prior_mean=0, prior_sd=1):
    """
    EAP Î¸ ì¶”ì •

    Args:
        responses: [1, 0, 1, 1, ...] (ì •ë‹µ ì—¬ë¶€)
        items: [{'a': 1.2, 'b': 0.5, 'c': 0.25}, ...]
        prior_mean: Prior í‰ê·  (0)
        prior_sd: Prior í‘œì¤€í¸ì°¨ (1)

    Returns:
        (theta, se)
    """
    def posterior_numerator(theta):
        # Prior: N(0, 1)
        prior = norm.pdf(theta, prior_mean, prior_sd)

        # Likelihood
        likelihood = 1.0
        for response, item in zip(responses, items):
            p = three_pl_probability(theta, item['a'], item['b'], item['c'])
            likelihood *= (p if response else (1 - p))

        return theta * likelihood * prior

    def posterior_denominator(theta):
        prior = norm.pdf(theta, prior_mean, prior_sd)
        likelihood = 1.0
        for response, item in zip(responses, items):
            p = three_pl_probability(theta, item['a'], item['b'], item['c'])
            likelihood *= (p if response else (1 - p))

        return likelihood * prior

    # Numerical integration (-4Ïƒ ~ +4Ïƒ)
    numerator, _ = quad(posterior_numerator, -4, 4, limit=100)
    denominator, _ = quad(posterior_denominator, -4, 4, limit=100)

    theta_eap = numerator / denominator if denominator != 0 else 0.0

    # SE ê³„ì‚°: Var[Î¸|X] = E[Î¸Â²|X] - (E[Î¸|X])Â²
    def posterior_second_moment(theta):
        prior = norm.pdf(theta, prior_mean, prior_sd)
        likelihood = 1.0
        for response, item in zip(responses, items):
            p = three_pl_probability(theta, item['a'], item['b'], item['c'])
            likelihood *= (p if response else (1 - p))
        return (theta ** 2) * likelihood * prior

    second_moment, _ = quad(posterior_second_moment, -4, 4, limit=100)
    variance = (second_moment / denominator) - (theta_eap ** 2)
    se = np.sqrt(variance) if variance > 0 else 0.3

    return theta_eap, se

# ì‚¬ìš© ì˜ˆì‹œ
items_data = [
    {'a': 1.5, 'b': -1.0, 'c': 0.25},
    {'a': 1.2, 'b': 0.5, 'c': 0.25},
    # ... 8ê°œ (Stage 1)
]
student_responses = [1, 0, 1, 1, 0, 1, 0, 1]  # 8ê°œ

theta, se = eap_estimate(student_responses, items_data)
print(f"Î¸ = {theta:.3f}, SE = {se:.3f}")
# Output: Î¸ = 0.235, SE = 0.752
```

#### 5.2.2 IRT íŒŒë¼ë¯¸í„° ìº˜ë¦¬ë¸Œë ˆì´ì…˜ (GIRTH)

**ìš”êµ¬ì‚¬í•­**:
- [x] íŒŒì¼ëŸ¿ ë°ì´í„° 200-500ëª… ìˆ˜ì§‘
- [x] GIRTH ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ 3PL MML(Marginal Maximum Likelihood) ì¶”ì •
- [x] ë¶€ì í•© ë¬¸í•­ í”Œë˜ê·¸ (Point-biserial < 0.2)
- [x] ë¶„ê¸°ë³„ ì¬ìº˜ë¦¬ë¸Œë ˆì´ì…˜

**êµ¬í˜„ (Python + GIRTH)**:
```python
from girth import threepl_mml
import numpy as np
import pandas as pd

# íŒŒì¼ëŸ¿ ë°ì´í„°: Nëª… í•™ìƒ Ã— Mê°œ ë¬¸í•­ (0/1 í–‰ë ¬)
# responses.shape = (200, 300)
responses = np.array([
    [1, 0, 1, 1, 0, ...],  # í•™ìƒ 1
    [1, 1, 1, 0, 0, ...],  # í•™ìƒ 2
    # ... 200ëª…
])

# 3PL MML ì¶”ì •
discrimination, difficulty, guessing = threepl_mml(responses)

# ê²°ê³¼ ì €ì¥
calibration_results = []
for i, (a, b, c) in enumerate(zip(discrimination, difficulty, guessing)):
    # Point-biserial ê³„ì‚° (ë¬¸í•­-ì´ì  ìƒê´€)
    item_scores = responses[:, i]
    total_scores = responses.sum(axis=1)
    point_biserial = np.corrcoef(item_scores, total_scores)[0, 1]

    # ë¶€ì í•© ë¬¸í•­ í”Œë˜ê·¸
    status = 'active'
    if point_biserial < 0.2:
        status = 'flagged'  # ë³€ë³„ë ¥ ë¶€ì¡±
    elif a < 0.5 or a > 2.5:
        status = 'flagged'  # ë³€ë³„ë„ ë²”ìœ„ ì´ˆê³¼
    elif abs(b) > 3:
        status = 'flagged'  # ë‚œì´ë„ ë²”ìœ„ ì´ˆê³¼

    calibration_results.append({
        'item_id': i + 1,
        'discrimination': float(a),
        'difficulty': float(b),
        'guessing': float(c),
        'point_biserial': float(point_biserial),
        'status': status
    })

# DataFrameìœ¼ë¡œ ë³€í™˜
df = pd.DataFrame(calibration_results)
print(df.head())

# DB ì—…ë°ì´íŠ¸
for row in df.itertuples():
    update_item_parameters(
        item_id=row.item_id,
        a=row.discrimination,
        b=row.difficulty,
        c=row.guessing,
        status=row.status
    )

# ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì´ë ¥ ì €ì¥
save_calibration_run({
    'method': '3PL-MML',
    'sample_size': responses.shape[0],
    'items_updated': len(df),
    'items_flagged': len(df[df.status == 'flagged']),
    'log_likelihood': calculate_log_likelihood(responses, df),
    'notes': 'Quarterly recalibration 2025 Q1'
})
```

**API ì—”ë“œí¬ì¸íŠ¸ (ê´€ë¦¬ììš©)**:
```
POST /api/admin/calibrate
Request Body:
{
  "pilot_data_file": "pilot_responses_202501.csv",
  "method": "3PL-MML"
}

Response:
{
  "calibration_run_id": 15,
  "items_updated": 300,
  "items_flagged": 12,
  "flagged_items": [23, 45, 67, ...],
  "log_likelihood": -45231.2,
  "aic": 90600.4,
  "bic": 91254.7
}
```

### 5.3 Diagnostic Engine

#### 5.3.1 10ë ˆë²¨ ì ˆëŒ€ ê¸°ì¤€ ë³€í™˜

**ìš”êµ¬ì‚¬í•­**:
- [x] Î¸ â†’ 10ë ˆë²¨ ë³€í™˜ (Bookmark ë°©ì‹)
- [x] ê° ë ˆë²¨ ì ˆëŒ€ ê¸°ì¤€ ì„¤ëª…
- [x] ë ˆë²¨ ì»· ì ìˆ˜ ì„¤ì • (ì „ë¬¸ê°€ íŒ¨ë„)

**ë ˆë²¨ ì •ì˜**:
```python
LEVEL_CUTS = {
    1: (-float('inf'), -2.5),  # ê¸°ì´ˆ 1
    2: (-2.5, -2.0),           # ê¸°ì´ˆ 2
    3: (-2.0, -1.5),           # ê¸°ì´ˆ 3
    4: (-1.5, -1.0),           # ì´ˆê¸‰ 1
    5: (-1.0, -0.5),           # ì´ˆê¸‰ 2
    6: (-0.5, 0.0),            # ì¤‘ê¸‰ 1
    7: (0.0, 0.5),             # ì¤‘ê¸‰ 2
    8: (0.5, 1.0),             # ì¤‘ìƒê¸‰
    9: (1.0, 1.5),             # ê³ ê¸‰ 1
    10: (1.5, float('inf'))    # ê³ ê¸‰ 2 (ëŒ€í•™ì›)
}

LEVEL_DESCRIPTIONS = {
    1: "ê¸°ì´ˆ ë‹¨ì–´(sight words)ì™€ ì§§ì€ ë¬¸ì¥(3-5ë‹¨ì–´)ì„ ì´í•´í•  ìˆ˜ ìˆìŒ. ì£¼ë¡œ ê·¸ë¦¼ì±… ìˆ˜ì¤€.",
    2: "ì¼ìƒì ì¸ ì£¼ì œì˜ ê°„ë‹¨í•œ ê¸€(100-200ë‹¨ì–´)ì„ ì½ì„ ìˆ˜ ìˆìŒ. ê¸°ë³¸ í˜„ì¬/ê³¼ê±° ì‹œì œ ì´í•´.",
    3: "ì¹œìˆ™í•œ ì£¼ì œì˜ ì„¤ëª…ë¬¸ì„ ëŒ€ì²´ë¡œ ì´í•´í•¨. ë¬¸ì¥ êµ¬ì¡°ê°€ ë‹¤ì†Œ ë³µì¡í•´ì ¸ë„(ì¢…ì†ì ˆ 1ê°œ) íŒŒì•… ê°€ëŠ¥.",
    4: "ë‹¤ì–‘í•œ ì£¼ì œì˜ ê¸€(300-500ë‹¨ì–´)ì„ ì½ê³  ì£¼ìš” ë‚´ìš©ê³¼ ì„¸ë¶€ì‚¬í•­ì„ êµ¬ë¶„í•¨. ì£¼ì œ ì¶”ë¡  ê°€ëŠ¥.",
    5: "ë…¼ë¦¬ì  ê´€ê³„(ì¸ê³¼, ë¹„êµ)ë¥¼ íŒŒì•…í•˜ë©° ì¤‘ê¸‰ ìˆ˜ì¤€ì˜ ê¸€ì„ ë…í•´í•¨. í•„ìì˜ ì˜ë„ ì´í•´ ì‹œì‘.",
    6: "ë³µì¡í•œ êµ¬ì¡°ì˜ ê¸€(ë‹¤ë‹¨ë½, 500ë‹¨ì–´+)ì—ì„œ í•¨ì¶•ì  ì˜ë¯¸ì™€ ì¶”ë¡ ì„ í•  ìˆ˜ ìˆìŒ. ë¬¸í•™ì  í‘œí˜„ ì´í•´.",
    7: "í•™ìˆ ì  í…ìŠ¤íŠ¸(êµê³¼ì„œ, ë…¼ë¬¸ ì´ˆë¡)ë¥¼ ì½ê³  ë¹„íŒì ìœ¼ë¡œ ë¶„ì„í•¨. ë…¼ì¦ êµ¬ì¡° íŒŒì•….",
    8: "ì „ë¬¸ì  ë‚´ìš©ì˜ ê¸€(ì „ë¬¸ ì„œì , í•™ìˆ ë…¼ë¬¸)ì„ ê¹Šì´ ì´í•´í•˜ê³  ì¢…í•©í•¨. ë³µì¡í•œ ì–´íœ˜ì™€ êµ¬ë¬¸ ëŠ¥ìˆ™í•˜ê²Œ ì²˜ë¦¬.",
    9: "ê³ ê¸‰ ì–´íœ˜(í•™ìˆ  ì „ë¬¸ìš©ì–´)ì™€ ë³µì¡í•œ ë¬¸ì¥ êµ¬ì¡°(ë‹¤ì¸µ ì¢…ì†ì ˆ)ë¥¼ ëŠ¥ìˆ™í•˜ê²Œ ì²˜ë¦¬í•¨. ë¯¸ë¬˜í•œ ë‰˜ì•™ìŠ¤ íŒŒì•….",
    10: "ëŒ€í•™ì› ìˆ˜ì¤€ì˜ í•™ìˆ  í…ìŠ¤íŠ¸ë¥¼ ì™„ì „íˆ ì´í•´í•¨. í•™ë¬¸ ë¶„ì•¼ë³„ ì „ë¬¸ í…ìŠ¤íŠ¸ë„ ë…í•´ ê°€ëŠ¥."
}

def theta_to_level(theta):
    """Î¸ë¥¼ 10ë ˆë²¨ë¡œ ë³€í™˜"""
    for level, (low, high) in LEVEL_CUTS.items():
        if low <= theta < high:
            return level
    return 10  # Î¸ â‰¥ 1.5
```

**API ì‘ë‹µ ì˜ˆì‹œ**:
```json
{
  "diagnostic_level": 7,
  "description": "í•™ìˆ ì  í…ìŠ¤íŠ¸(êµê³¼ì„œ, ë…¼ë¬¸ ì´ˆë¡)ë¥¼ ì½ê³  ë¹„íŒì ìœ¼ë¡œ ë¶„ì„í•¨. ë…¼ì¦ êµ¬ì¡° íŒŒì•….",
  "theta": 0.32,
  "se": 0.28,
  "confidence_interval_95": [0.26, 0.38]
}
```

#### 5.3.2 Lexile/AR ì¶”ì • (ML ëª¨ë¸)

**ìš”êµ¬ì‚¬í•­**:
- [x] ì§€ë¬¸ í…ìŠ¤íŠ¸ íŠ¹ì§• ì¶”ì¶œ (Textstat, Wordfreq, spaCy)
- [x] Gradient Boosting íšŒê·€ ëª¨ë¸ í•™ìŠµ (RÂ² â‰¥ 0.85)
- [x] ê³µê°œ ì½”í¼ìŠ¤ì—ì„œ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ (500+ í…ìŠ¤íŠ¸)
- [x] Lexile â†” AR ë³€í™˜í‘œ ì ìš©

**í…ìŠ¤íŠ¸ íŠ¹ì§• ì¶”ì¶œ**:
```python
import textstat
from wordfreq import zipf_frequency
import spacy
import numpy as np

nlp = spacy.load('en_core_web_sm')

def extract_text_features(passage_text):
    """
    ì§€ë¬¸ì—ì„œ Lexile/AR ì˜ˆì¸¡ì— í•„ìš”í•œ íŠ¹ì§• ì¶”ì¶œ

    Returns:
        dict: íŠ¹ì§• ë²¡í„° (12ê°œ feature)
    """
    doc = nlp(passage_text)
    words = [token.text for token in doc if token.is_alpha]
    sentences = list(doc.sents)

    # ë¬¸ì¥ ê¸¸ì´
    sentence_lengths = [len(list(sent)) for sent in sentences]
    mean_sentence_length = np.mean(sentence_lengths)
    max_sentence_length = np.max(sentence_lengths)

    # ë‹¨ì–´ ê¸¸ì´
    word_lengths = [len(w) for w in words]
    mean_word_length = np.mean(word_lengths)

    # ì–´íœ˜ ë¹ˆë„ (Zipf scale: 1-7, 7=ë§¤ìš° í”í•¨)
    zipf_scores = [zipf_frequency(w.lower(), 'en') for w in words]
    mean_zipf = np.mean(zipf_scores)
    low_freq_ratio = sum(1 for z in zipf_scores if z < 3.0) / len(words)

    # Type-Token Ratio (ì–´íœ˜ ë‹¤ì–‘ì„±)
    ttr = len(set(words)) / len(words)

    # ì „í†µì  ê°€ë…ì„± ì§€í‘œ
    flesch_kincaid = textstat.flesch_kincaid_grade(passage_text)
    dale_chall = textstat.dale_chall_readability_score(passage_text)

    # êµ¬ë¬¸ ë³µì¡ë„
    subordinate_clauses = sum(1 for token in doc if token.dep_ == 'mark')
    subordinate_ratio = subordinate_clauses / len(words)

    # Passive voice ë¹„ìœ¨
    passive_count = sum(1 for sent in doc.sents
                       if any(token.dep_ == 'nsubjpass' for token in sent))
    passive_ratio = passive_count / len(sentences)

    return {
        'mean_sentence_length': mean_sentence_length,
        'max_sentence_length': max_sentence_length,
        'mean_word_length': mean_word_length,
        'mean_zipf': mean_zipf,
        'low_freq_ratio': low_freq_ratio,
        'ttr': ttr,
        'flesch_kincaid': flesch_kincaid,
        'dale_chall': dale_chall,
        'subordinate_ratio': subordinate_ratio,
        'passive_ratio': passive_ratio,
        'word_count': len(words),
        'sentence_count': len(sentences)
    }
```

**ML ëª¨ë¸ í•™ìŠµ**:
```python
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import cross_val_score, train_test_split
import pandas as pd

# í•™ìŠµ ë°ì´í„° ë¡œë“œ (ê³µê°œ ì½”í¼ìŠ¤ + Lexile ë¼ë²¨)
# ì˜ˆ: CommonLit, Project Gutenberg, Newsela ë“±
training_data = pd.read_csv('lexile_training_corpus.csv')
# columns: ['text', 'lexile', 'source']

# íŠ¹ì§• ì¶”ì¶œ
X_features = []
y_lexile = []

for idx, row in training_data.iterrows():
    features = extract_text_features(row['text'])
    X_features.append(list(features.values()))
    y_lexile.append(row['lexile'])

X = np.array(X_features)
y = np.array(y_lexile)

# Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Gradient Boosting ëª¨ë¸
lexile_model = GradientBoostingRegressor(
    n_estimators=200,
    learning_rate=0.05,
    max_depth=5,
    min_samples_split=10,
    min_samples_leaf=4,
    subsample=0.8,
    random_state=42
)

lexile_model.fit(X_train, y_train)

# êµì°¨ê²€ì¦ (RÂ² â‰¥ 0.85 ëª©í‘œ)
cv_scores = cross_val_score(
    lexile_model, X_train, y_train,
    cv=5, scoring='r2'
)
print(f"Lexile ëª¨ë¸ RÂ²: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}")
# Output: Lexile ëª¨ë¸ RÂ²: 0.872 Â± 0.023

# Test set ì„±ëŠ¥
from sklearn.metrics import mean_absolute_error, r2_score

y_pred = lexile_model.predict(X_test)
print(f"Test RÂ²: {r2_score(y_test, y_pred):.3f}")
print(f"Test MAE: {mean_absolute_error(y_test, y_pred):.1f}L")
# Output: Test RÂ²: 0.865
# Output: Test MAE: 67.3L

# ëª¨ë¸ ì €ì¥
import joblib
joblib.dump(lexile_model, 'models/lexile_estimator.pkl')
```

**Lexile â†” AR ë³€í™˜**:
```python
# ê³µê°œëœ ë³€í™˜í‘œ (ì¶œì²˜: Arbordale Publishing, Renaissance Learning)
LEXILE_TO_AR_TABLE = {
    100: 0.5, 200: 1.5, 300: 2.0, 400: 2.5, 500: 3.0,
    600: 3.5, 700: 4.0, 800: 4.5, 900: 5.0, 1000: 5.5,
    1100: 6.0, 1200: 7.0, 1300: 8.0, 1400: 9.0, 1500: 10.0,
    1600: 11.0, 1700: 12.0
}

def lexile_to_ar(lexile):
    """Lexile ì ìˆ˜ë¥¼ AR ë“±ê¸‰ìœ¼ë¡œ ë³€í™˜ (ì„ í˜• ë³´ê°„)"""
    if lexile <= 100:
        return 0.5
    if lexile >= 1700:
        return 12.0

    # ì„ í˜• ë³´ê°„
    for lex_low, lex_high in zip(
        sorted(LEXILE_TO_AR_TABLE.keys())[:-1],
        sorted(LEXILE_TO_AR_TABLE.keys())[1:]
    ):
        if lex_low <= lexile < lex_high:
            ar_low = LEXILE_TO_AR_TABLE[lex_low]
            ar_high = LEXILE_TO_AR_TABLE[lex_high]
            ratio = (lexile - lex_low) / (lex_high - lex_low)
            return ar_low + ratio * (ar_high - ar_low)

    return 6.0  # fallback

# ì‚¬ìš© ì˜ˆì‹œ
estimated_lexile = lexile_model.predict([passage_features])[0]
estimated_ar = lexile_to_ar(estimated_lexile)

print(f"Estimated Lexile: {estimated_lexile:.0f}L")
print(f"Estimated AR: {estimated_ar:.1f}")
# Output: Estimated Lexile: 950L
# Output: Estimated AR: 5.2
```

**API ì—”ë“œí¬ì¸íŠ¸**:
```
POST /api/diagnostic/lexile-ar
Request Body:
{
  "passage_texts": [
    "First passage content...",
    "Second passage content...",
    // ... í•™ìƒì´ ì½ì€ ë…í•´ ì§€ë¬¸ë“¤
  ],
  "student_theta": 0.62
}

Response:
{
  "average_lexile": 950,
  "average_ar": 5.2,
  "passages": [
    {"text_id": 1, "lexile": 920, "ar": 5.0},
    {"text_id": 2, "lexile": 980, "ar": 5.4}
  ],
  "recommendation": "ì´ í•™ìƒì€ Lexile 950L ìˆ˜ì¤€ìœ¼ë¡œ, 5í•™ë…„ ì¤‘ë°˜~6í•™ë…„ ì´ˆë°˜ ìˆ˜ì¤€ì…ë‹ˆë‹¤."
}
```

#### 5.3.3 ì–´íœ˜ ì‚¬ì´ì¦ˆ ì¶”ì • (VST + ê°€ì§œì–´)

**ìš”êµ¬ì‚¬í•­**:
- [x] Paul Nation VST(Vocabulary Size Test) ë°©ì‹ ì ìš©
- [x] ë¹ˆë„ ë°´ë“œë³„ ë¬¸í•­ êµ¬ì„± (1k, 2k, 4k, 6k, 8k, 10k, 14k)
- [x] ê° ë°´ë“œ 2ë¬¸í•­ì”© (ì´ 14ë¬¸í•­)
- [x] ê°€ì§œì–´(Pseudo-words) 3ê°œ í¬í•¨ (ê³¼ëŒ€ì‘ë‹µ ê°ì§€)
- [x] IRT ë³´ì • ì ìš©

**ë¹ˆë„ ë°´ë“œ ì •ì˜**:
```python
# BNC/COCA ê¸°ì¤€ ë¹ˆë„ ë°´ë“œ
FREQUENCY_BANDS = {
    '1k': (0, 1000),       # ê°€ì¥ í”í•œ 1,000ë‹¨ì–´
    '2k': (1000, 2000),
    '4k': (2000, 4000),
    '6k': (4000, 6000),
    '8k': (6000, 8000),
    '10k': (8000, 10000),
    '14k': (10000, 14000)  # í•™ìˆ /ì „ë¬¸ ì–´íœ˜
}
```

**ë¬¸í•­ ì˜ˆì‹œ**:
```json
{
  "item_id": 250,
  "stem": "What does 'revolutionary' mean?",
  "options": [
    "A) causing a big change",
    "B) moving in circles",
    "C) very old",
    "D) I don't know this word"
  ],
  "correct_answer": "A",
  "domain": "Vocabulary",
  "word": "revolutionary",
  "frequency_band": "4k",
  "is_pseudo": false
}

{
  "item_id": 267,
  "stem": "What does 'blinsary' mean?",
  "options": [
    "A) shining brightly",
    "B) making a loud noise",
    "C) moving slowly",
    "D) I don't know this word"
  ],
  "correct_answer": "D",
  "domain": "Vocabulary",
  "word": "blinsary",
  "frequency_band": null,
  "is_pseudo": true  // ê°€ì§œì–´!
}
```

**ì–´íœ˜ëŸ‰ ì¶”ì • ì•Œê³ ë¦¬ì¦˜**:
```python
def estimate_vocabulary_size(responses, items):
    """
    Paul Nation VST ë°©ì‹ + IRT ë³´ì •

    Args:
        responses: [(item_id, is_correct), ...]
        items: [{word, band, is_pseudo, difficulty}, ...]

    Returns:
        {
            'total': int,  # ì´ ì–´íœ˜ëŸ‰
            'confidence': str,  # 'High' or 'Low'
            'by_band': dict,  # ë°´ë“œë³„ ì¶”ì •ì¹˜
            'pseudo_alarm_rate': float  # ê°€ì§œì–´ ì˜¤íƒë¥ 
        }
    """
    # 1. ê°€ì§œì–´ ì²´í¬ (ê³¼ëŒ€ì‘ë‹µ í˜ë„í‹°)
    pseudo_items = [item for item in items if item.get('is_pseudo')]
    pseudo_responses = [
        r for r, item in zip(responses, items)
        if item.get('is_pseudo')
    ]
    pseudo_correct = sum(pseudo_responses)

    if pseudo_correct >= 2:
        # ê³¼ëŒ€ì‘ë‹µì â†’ ì‹ ë¢°ë„ ë‚®ì¶¤
        confidence = 'Low'
        penalty = 0.7  # 30% í˜ë„í‹°
    elif pseudo_correct == 1:
        confidence = 'Medium'
        penalty = 0.9
    else:
        confidence = 'High'
        penalty = 1.0

    # 2. ë°´ë“œë³„ ì •ë‹µë¥ 
    band_scores = {}
    for band_name, (start, end) in FREQUENCY_BANDS.items():
        band_items = [
            item for item in items
            if item.get('band') == band_name and not item.get('is_pseudo')
        ]
        band_responses = [
            r for r, item in zip(responses, items)
            if item in band_items
        ]

        if band_responses:
            # ë‹¨ìˆœ ì •ë‹µë¥ 
            correct_rate = sum(band_responses) / len(band_responses)

            # IRT ë³´ì • (ë‚œì´ë„ ê³ ë ¤)
            mean_difficulty = np.mean([item['difficulty'] for item in band_items])
            # ì–´ë ¤ìš´ ë¬¸í•­ì¼ìˆ˜ë¡ ì‹¤ì œ ì–´íœ˜ëŸ‰ì€ ë” ë†’ê²Œ ì¶”ì •
            irt_correction = 1.0 + (mean_difficulty * 0.1)

            band_size = end - start
            band_scores[band_name] = correct_rate * band_size * irt_correction
        else:
            band_scores[band_name] = 0

    # 3. ì´ ì–´íœ˜ëŸ‰ ì¶”ì •
    total_vocab = sum(band_scores.values()) * penalty

    return {
        'total': int(total_vocab),
        'confidence': confidence,
        'by_band': band_scores,
        'pseudo_alarm_rate': pseudo_correct / len(pseudo_items) if pseudo_items else 0
    }

# ì‚¬ìš© ì˜ˆì‹œ
vocab_items_data = [
    {'word': 'book', 'band': '1k', 'is_pseudo': False, 'difficulty': -2.0},
    {'word': 'democracy', 'band': '4k', 'is_pseudo': False, 'difficulty': 0.5},
    # ... 14ê°œ (ì§„ì§œ ë‹¨ì–´)
    {'word': 'blinsary', 'band': None, 'is_pseudo': True, 'difficulty': 0.0},
    # ... 3ê°œ (ê°€ì§œì–´)
]
student_vocab_responses = [1, 1, 1, 0, 1, ..., 0, 0, 0]  # 17ê°œ

vocab_result = estimate_vocabulary_size(student_vocab_responses, vocab_items_data)
print(vocab_result)
# Output:
# {
#   'total': 6200,
#   'confidence': 'High',
#   'by_band': {
#     '1k': 980, '2k': 920, '4k': 1580, '6k': 1200,
#     '8k': 800, '10k': 520, '14k': 200
#   },
#   'pseudo_alarm_rate': 0.0
# }
```

**API ì‘ë‹µ ì˜ˆì‹œ**:
```json
{
  "vocabulary_size": 6200,
  "confidence": "High",
  "by_band": {
    "1k": 980,
    "2k": 920,
    "4k": 1580,
    "6k": 1200,
    "8k": 800,
    "10k": 520,
    "14k": 200
  },
  "interpretation": "ì´ í•™ìƒì€ ì•½ 6,200ë‹¨ì–´ë¥¼ ì•Œê³  ìˆì„ ê²ƒìœ¼ë¡œ ì¶”ì •ë©ë‹ˆë‹¤. ì´ëŠ” ì¤‘í•™êµ 2-3í•™ë…„ ìˆ˜ì¤€ì…ë‹ˆë‹¤.",
  "pseudo_alarm_rate": 0.0,
  "warning": null
}
```

#### 5.3.4 ë„ë©”ì¸ë³„ ê°•ì•½ì  ë¶„ì„

**ìš”êµ¬ì‚¬í•­**:
- [x] ë¬¸ë²•(Grammar) / ì–´íœ˜(Vocabulary) / ë…í•´(Reading) ì •ë‹µë¥  ê³„ì‚°
- [x] ê° ë„ë©”ì¸ë³„ Î¸ ì¶”ì • (ë¶„ë¦¬)
- [x] ë ˆì´ë” ì°¨íŠ¸ìš© ë°ì´í„° ì œê³µ

**êµ¬í˜„**:
```python
def analyze_domain_performance(responses, items):
    """
    ë„ë©”ì¸ë³„ ê°•ì•½ì  ë¶„ì„

    Returns:
        {
            'grammar': {'score': 0.75, 'theta': 0.3, 'se': 0.35},
            'vocabulary': {'score': 0.68, 'theta': 0.1, 'se': 0.38},
            'reading': {'score': 0.72, 'theta': 0.25, 'se': 0.36}
        }
    """
    domains = ['Grammar', 'Vocabulary', 'Reading']
    results = {}

    for domain in domains:
        # í•´ë‹¹ ë„ë©”ì¸ ë¬¸í•­ë§Œ í•„í„°ë§
        domain_items = [item for item in items if item['domain'] == domain]
        domain_responses = [
            r for r, item in zip(responses, items)
            if item['domain'] == domain
        ]

        # ì •ë‹µë¥ 
        score = sum(domain_responses) / len(domain_responses) if domain_responses else 0

        # ë„ë©”ì¸ë³„ Î¸ ì¶”ì • (EAP)
        theta, se = eap_estimate(domain_responses, domain_items)

        results[domain.lower()] = {
            'score': round(score, 2),
            'theta': round(theta, 2),
            'se': round(se, 2),
            'item_count': len(domain_responses)
        }

    return results

# ì‚¬ìš© ì˜ˆì‹œ
domain_analysis = analyze_domain_performance(all_responses, all_items)
print(domain_analysis)
# Output:
# {
#   'grammar': {'score': 0.75, 'theta': 0.30, 'se': 0.35, 'item_count': 13},
#   'vocabulary': {'score': 0.68, 'theta': 0.10, 'se': 0.38, 'item_count': 14},
#   'reading': {'score': 0.72, 'theta': 0.25, 'se': 0.36, 'item_count': 13}
# }
```

**API ì‘ë‹µ ì˜ˆì‹œ**:
```json
{
  "domains": {
    "grammar": {
      "score": 0.75,
      "theta": 0.30,
      "se": 0.35,
      "level": "ì¤‘ê¸‰ 2",
      "strength": "ìƒ",
      "feedback": "ë¬¸ë²• ì‹¤ë ¥ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤. ì‹œì œì™€ íƒœ ë³€í™˜ì„ ì •í™•í•˜ê²Œ ì´í•´í•˜ê³  ìˆìŠµë‹ˆë‹¤."
    },
    "vocabulary": {
      "score": 0.68,
      "theta": 0.10,
      "se": 0.38,
      "level": "ì¤‘ê¸‰ 1",
      "strength": "ì¤‘",
      "feedback": "ì–´íœ˜ë ¥ì€ ì¤‘ê¸‰ ìˆ˜ì¤€ì…ë‹ˆë‹¤. í•™ìˆ  ì–´íœ˜(4k-6k ë°´ë“œ) í•™ìŠµì„ ê¶Œì¥í•©ë‹ˆë‹¤."
    },
    "reading": {
      "score": 0.72,
      "theta": 0.25,
      "se": 0.36,
      "level": "ì¤‘ê¸‰ 2",
      "strength": "ì¤‘ìƒ",
      "feedback": "ë…í•´ë ¥ì´ ì–‘í˜¸í•©ë‹ˆë‹¤. ë…¼ì¦ë¬¸ê³¼ ì„¤ëª…ë¬¸ì˜ êµ¬ì¡° íŒŒì•…ì— ê°•ì ì´ ìˆìŠµë‹ˆë‹¤."
    }
  },
  "overall_strength": "ë¬¸ë²• > ë…í•´ > ì–´íœ˜",
  "recommendation": "ì–´íœ˜ë ¥ í–¥ìƒì— ì§‘ì¤‘í•˜ë©´ ì „ì²´ ì‹¤ë ¥ì´ ë¹ ë¥´ê²Œ í–¥ìƒë  ê²ƒì…ë‹ˆë‹¤."
}
```

---

## UI/UX ì„¤ê³„

### 6.1 ì‚¬ìš©ì í”Œë¡œìš° (User Flow)

```
[í•™ìƒ ë¡œê·¸ì¸]
    â†“
[í…ŒìŠ¤íŠ¸ ì„ íƒ: "ì˜ì–´ ëŠ¥ë ¥ ì§„ë‹¨"]
    â†“
[ì•ˆë‚´ í™”ë©´]
  - ì´ 40ë¬¸í•­, ì•½ 20-30ë¶„ ì†Œìš”
  - ê° ë‹¨ê³„ë§ˆë‹¤ ë‚œì´ë„ê°€ ì¡°ì •ë¨
  - ì¤‘ê°„ ì €ì¥ ë¶ˆê°€, í•œ ë²ˆì— ì™„ë£Œ í•„ìš”
  - [í…ŒìŠ¤íŠ¸ ì‹œì‘] ë²„íŠ¼
    â†“
[Stage 1: 8ë¬¸í•­]
  - Progress: 1/40 ~ 8/40
  - ë¬¸í•­ í‘œì‹œ (1ë¬¸í•­ì”©)
  - 4ì§€ì„ ë‹¤ + [ë‹¤ìŒ] ë²„íŠ¼
  - ì‘ë‹µ ì‹œê°„ ìë™ ê¸°ë¡
    â†“ (8ë¬¸í•­ ì™„ë£Œ)
[ë¡œë”© í™”ë©´]
  - "ë‹¹ì‹ ì˜ ì‹¤ë ¥ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."
  - Î¸â‚ ì¶”ì • + íŠ¸ë™ ê²°ì • (1ì´ˆ)
    â†“
[Stage 2: 16ë¬¸í•­]
  - Progress: 9/40 ~ 24/40
  - ì„ íƒëœ íŠ¸ë™(Low/Med/High)ì— ë§ëŠ” ë¬¸í•­
  - ë™ì¼í•œ UI
    â†“ (16ë¬¸í•­ ì™„ë£Œ)
[ë¡œë”© í™”ë©´]
  - "ê±°ì˜ ë‹¤ ì™”ìŠµë‹ˆë‹¤..."
  - Î¸â‚‚ ì¶”ì • + ì„œë¸ŒíŠ¸ë™ ê²°ì • (1ì´ˆ)
    â†“
[Stage 3: 16ë¬¸í•­]
  - Progress: 25/40 ~ 40/40
  - ì„ íƒëœ ì„œë¸ŒíŠ¸ë™ì— ë§ëŠ” ë¬¸í•­
  - ë™ì¼í•œ UI
    â†“ (40ë¬¸í•­ ì™„ë£Œ)
[ìµœì¢… ë¶„ì„ í™”ë©´]
  - "ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”"
  - Î¸ ìµœì¢… ì¶”ì •, Lexile/AR ì˜ˆì¸¡, ì–´íœ˜ëŸ‰ ì¶”ì • (2-3ì´ˆ)
    â†“
[ë¦¬í¬íŠ¸ í™”ë©´]
  - 10ë ˆë²¨ + ì„¤ëª…
  - Lexile/AR ì ìˆ˜
  - ì–´íœ˜ ì‚¬ì´ì¦ˆ
  - ë„ë©”ì¸ë³„ ë ˆì´ë” ì°¨íŠ¸
  - [PDF ë‹¤ìš´ë¡œë“œ] [ë‹¤ì‹œ ë³´ê¸°] ë²„íŠ¼
```

### 6.2 í™”ë©´ ì„¤ê³„ (Wireframes)

#### 6.2.1 ì•ˆë‚´ í™”ë©´ (Intro Screen)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  [Logo] ì˜ì–´ ëŠ¥ë ¥ ì§„ë‹¨ í…ŒìŠ¤íŠ¸                              â”‚
â”‚                                                          â”‚
â”‚  ğŸ“‹ í…ŒìŠ¤íŠ¸ ì•ˆë‚´                                           â”‚
â”‚                                                          â”‚
â”‚  âœ… ì´ 40ë¬¸í•­ (ì•½ 20-30ë¶„ ì†Œìš”)                           â”‚
â”‚  âœ… 3ë‹¨ê³„ë¡œ êµ¬ì„± (ê° ë‹¨ê³„ë§ˆë‹¤ ë‚œì´ë„ ìë™ ì¡°ì •)              â”‚
â”‚  âœ… ë¬¸ë²•, ì–´íœ˜, ë…í•´ í†µí•© í‰ê°€                             â”‚
â”‚  âœ… 10ë ˆë²¨ + Lexile/AR ì ìˆ˜ ì œê³µ                          â”‚
â”‚                                                          â”‚
â”‚  âš ï¸ ì£¼ì˜ì‚¬í•­                                             â”‚
â”‚  - ì¤‘ê°„ ì €ì¥ ë¶ˆê°€, í•œ ë²ˆì— ì™„ë£Œ í•„ìš”                       â”‚
â”‚  - ê° ë¬¸í•­ë‹¹ ì œí•œ ì‹œê°„ ì—†ìŒ (ì¶©ë¶„íˆ ìƒê°í•˜ì„¸ìš”)             â”‚
â”‚  - ëª¨ë¥´ëŠ” ë¬¸ì œë„ ì¶”ì¸¡í•˜ì—¬ ë‹µë³€ ê¶Œì¥                        â”‚
â”‚                                                          â”‚
â”‚                    [í…ŒìŠ¤íŠ¸ ì‹œì‘í•˜ê¸°]                      â”‚
â”‚                    [ë‚˜ì¤‘ì— í•˜ê¸°]                          â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 6.2.2 ë¬¸í•­ í™”ë©´ (Item Screen)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Progress: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  8/40 (20%)      â”‚
â”‚                                                          â”‚
â”‚  Question 8                                              â”‚
â”‚                                                          â”‚
â”‚  [ì§€ë¬¸ ì˜ì—­ - ë…í•´ ë¬¸í•­ì¸ ê²½ìš°ë§Œ í‘œì‹œ]                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ The Industrial Revolution marked a turning     â”‚     â”‚
â”‚  â”‚ point in human history. New machines and       â”‚     â”‚
â”‚  â”‚ factories transformed the way people lived     â”‚     â”‚
â”‚  â”‚ and worked...                                  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                          â”‚
â”‚  What is the main idea of this passage?                 â”‚
â”‚                                                          â”‚
â”‚  â—‹ A) Machines are dangerous to workers                â”‚
â”‚  â—‹ B) The Industrial Revolution changed society        â”‚
â”‚  â—‹ C) Factories were built in cities                   â”‚
â”‚  â—‹ D) People preferred traditional methods             â”‚
â”‚                                                          â”‚
â”‚                                        [ì´ì „] [ë‹¤ìŒ]     â”‚
â”‚                                                          â”‚
â”‚  Time elapsed: 00:35                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 6.2.3 ë¡œë”© í™”ë©´ (Stage Transition)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                          â”‚
â”‚                        ğŸ”„                                â”‚
â”‚                                                          â”‚
â”‚              ë‹¹ì‹ ì˜ ì‹¤ë ¥ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...                 â”‚
â”‚                                                          â”‚
â”‚                   â—â—â—â—‹â—‹â—‹â—‹â—‹                              â”‚
â”‚                                                          â”‚
â”‚         ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”. ë‹¤ìŒ ë‹¨ê³„ë¥¼ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤.   â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 6.2.4 ë¦¬í¬íŠ¸ í™”ë©´ (Report Screen)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  [Logo] ì˜ì–´ ëŠ¥ë ¥ ì§„ë‹¨ ê²°ê³¼                    [PDF ì €ì¥]  â”‚
â”‚                                                          â”‚
â”‚  í•™ìƒ: ê¹€ë¯¼ì§€                  ì‘ì‹œì¼: 2025-01-20         â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ğŸ“Š ì¢…í•© ë ˆë²¨                                      â”‚   â”‚
â”‚  â”‚                                                   â”‚   â”‚
â”‚  â”‚          â­ Level 7 (ì¤‘ê¸‰ 2)                      â”‚   â”‚
â”‚  â”‚                                                   â”‚   â”‚
â”‚  â”‚  í•™ìˆ ì  í…ìŠ¤íŠ¸(êµê³¼ì„œ, ë…¼ë¬¸ ì´ˆë¡)ë¥¼ ì½ê³           â”‚   â”‚
â”‚  â”‚  ë¹„íŒì ìœ¼ë¡œ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚  â”‚ Lexile      â”‚ AR ë“±ê¸‰     â”‚                          â”‚
â”‚  â”‚ 950L        â”‚ 5.2         â”‚                          â”‚
â”‚  â”‚             â”‚             â”‚                          â”‚
â”‚  â”‚ (Grade 5-6) â”‚ (5í•™ë…„ ìˆ˜ì¤€) â”‚                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ğŸ“š ì¶”ì • ì–´íœ˜ëŸ‰: ì•½ 6,200ë‹¨ì–´                      â”‚   â”‚
â”‚  â”‚                                                   â”‚   â”‚
â”‚  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 1k (980/1000)                    â”‚   â”‚
â”‚  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆï¿½ï¿½ï¿½â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ 2k (920/1000)                    â”‚   â”‚
â”‚  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 4k (1580/2000)                   â”‚   â”‚
â”‚  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 6k (1200/2000)                   â”‚   â”‚
â”‚  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 8k (800/2000)                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ğŸ“ˆ ë„ë©”ì¸ë³„ ë¶„ì„                                  â”‚   â”‚
â”‚  â”‚                                                   â”‚   â”‚
â”‚  â”‚             ë¬¸ë²•                                  â”‚   â”‚
â”‚  â”‚              â­                                   â”‚   â”‚
â”‚  â”‚            â­   â­                                â”‚   â”‚
â”‚  â”‚          â­       â­                              â”‚   â”‚
â”‚  â”‚   ì–´íœ˜  â­           â­  ë…í•´                      â”‚   â”‚
â”‚  â”‚          â­       â­                              â”‚   â”‚
â”‚  â”‚            â­   â­                                â”‚   â”‚
â”‚  â”‚              â­                                   â”‚   â”‚
â”‚  â”‚                                                   â”‚   â”‚
â”‚  â”‚  - ë¬¸ë²•: 75% (ìƒ) - Î¸ = 0.30                     â”‚   â”‚
â”‚  â”‚  - ì–´íœ˜: 68% (ì¤‘) - Î¸ = 0.10                     â”‚   â”‚
â”‚  â”‚  - ë…í•´: 72% (ì¤‘ìƒ) - Î¸ = 0.25                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                          â”‚
â”‚  ğŸ’¡ ë§ì¶¤ í•™ìŠµ ì¶”ì²œ                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚  1. ì–´íœ˜ë ¥ í–¥ìƒì— ì§‘ì¤‘í•˜ì„¸ìš” (í˜„ì¬ ì•½ì  ì˜ì—­)             â”‚
â”‚     - ì¶”ì²œ ë„ì„œ: Charlotte's Web, Harry Potter 1      â”‚
â”‚     - 4k-6k ë°´ë“œ ë‹¨ì–´ í•™ìŠµ (í•™ìˆ  ì–´íœ˜)                  â”‚
â”‚                                                          â”‚
â”‚  2. ë…í•´ ì—°ìŠµ (Lexile 950L ìˆ˜ì¤€)                        â”‚
â”‚     - ì„¤ëª…ë¬¸/ë…¼ì¦ë¬¸ êµ¬ì¡° ë¶„ì„ ì—°ìŠµ                       â”‚
â”‚                                                          â”‚
â”‚                 [ë‹¤ì‹œ í…ŒìŠ¤íŠ¸í•˜ê¸°] [ì™„ë£Œ]                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.3 ì»´í¬ë„ŒíŠ¸ ì„¤ê³„ (Component Hierarchy)

```
<EnglishAdaptiveTest>
  â”œâ”€ <TestIntro>
  â”‚   â”œâ”€ <InfoCard>
  â”‚   â””â”€ <StartButton>
  â”‚
  â”œâ”€ <TestSession>
  â”‚   â”œâ”€ <ProgressBar current={8} total={40} />
  â”‚   â”œâ”€ <StageIndicator stage={1} />
  â”‚   â”œâ”€ <ItemDisplay>
  â”‚   â”‚   â”œâ”€ <Passage text={...} /> (ë…í•´ ë¬¸í•­ë§Œ)
  â”‚   â”‚   â”œâ”€ <QuestionStem stem={...} />
  â”‚   â”‚   â””â”€ <OptionList options={[...]} onSelect={...} />
  â”‚   â”œâ”€ <NavigationButtons>
  â”‚   â””â”€ <Timer />
  â”‚
  â”œâ”€ <StageTransitionLoading>
  â”‚   â”œâ”€ <Spinner />
  â”‚   â””â”€ <Message text="ë¶„ì„ ì¤‘..." />
  â”‚
  â””â”€ <ReportScreen>
      â”œâ”€ <LevelCard level={7} description={...} />
      â”œâ”€ <LexileARCard lexile={950} ar={5.2} />
      â”œâ”€ <VocabularyCard size={6200} byBand={...} />
      â”œâ”€ <DomainRadarChart data={...} />
      â”œâ”€ <RecommendationCard />
      â””â”€ <ActionButtons>
          â”œâ”€ <DownloadPDFButton />
          â””â”€ <RetakeButton />
```

### 6.4 ë°˜ì‘í˜• ë””ìì¸ (Responsive Design)

**Breakpoints**:
- **Desktop**: â‰¥ 1024px (ì£¼ íƒ€ê²Ÿ)
- **Tablet**: 768px ~ 1023px
- **Mobile**: < 768px (Phase 2)

**Desktop Layout**:
```
[Header (60px)]
[Content Area (calc(100vh - 120px))]
  - Max width: 900px
  - Center aligned
  - Padding: 40px
[Footer (60px)]
```

**Mobile Layout (Phase 2)**:
```
[Header (50px)]
[Content Area (calc(100vh - 100px))]
  - Full width
  - Padding: 20px
[Footer (50px)]
```

---

## ê°œë°œ ë¡œë“œë§µ

### 7.1 Phase 1: MVP (3ê°œì›”)

#### **Week 1-2: ì„¤ê³„ í™•ì • ë° í™˜ê²½ êµ¬ì¶•**

**Backend**:
- [x] DB ìŠ¤í‚¤ë§ˆ ìƒì„± (Supabase PostgreSQL)
- [x] FastAPI í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
- [x] GIRTH, NumPy, SciPy ë“± ì„¤ì¹˜
- [x] `/api/health` ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸

**Frontend**:
- [x] Next.js 14 í”„ë¡œì íŠ¸ ìƒì„±
- [x] Tailwind CSS ì„¤ì •
- [x] ì»´í¬ë„ŒíŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±

**ë¬¸ì„œí™”**:
- [x] MST ë¸”ë£¨í”„ë¦°íŠ¸ ì‘ì„± (ì—‘ì…€)
  - Stage 1: 8ë¬¸í•­ ë°°ì¹˜ ê³„íš
  - Stage 2: 48ë¬¸í•­ (Low/Med/High Ã— 16)
  - Stage 3: 132ë¬¸í•­ (9 subtracks Ã— ì•½ 15)

#### **Week 3-6: ë¬¸í•­ ì œì‘ (300ê°œ)**

**ì§€ë¬¸ ìˆ˜ì§‘/ì‘ì„±**:
- [x] 50ê°œ ì§€ë¬¸ (ìœ í˜•ë³„ ê· í˜•)
  - Expository: 15ê°œ
  - Argumentative: 10ê°œ
  - Narrative: 15ê°œ
  - Practical: 10ê°œ
- [x] Word count: 100-500ë‹¨ì–´
- [x] Textstatìœ¼ë¡œ Flesch-Kincaid í™•ì¸

**ë¬¸í•­ ì‘ì„±**:
- [x] ë¬¸ë²• 100ê°œ
  - Tenses: 30ê°œ
  - Subject-Verb Agreement: 20ê°œ
  - Active/Passive Voice: 15ê°œ
  - Modal Verbs: 15ê°œ
  - Conditionals: 10ê°œ
  - Others: 10ê°œ
- [x] ì–´íœ˜ 100ê°œ
  - ë¹ˆë„ ë°´ë“œë³„ (1k~14k) ê· í˜•
  - VST ë°©ì‹ 14ê°œ (ê° ë°´ë“œ 2ê°œ)
  - ê°€ì§œì–´ 3ê°œ
  - ì¼ë°˜ ì–´íœ˜ 83ê°œ
- [x] ë…í•´ 100ê°œ
  - Main Idea: 25ê°œ
  - Supporting Details: 25ê°œ
  - Inference: 25ê°œ
  - Author's Purpose: 15ê°œ
  - Vocabulary in Context: 10ê°œ

**ë¬¸í•­ ë©”íƒ€ë°ì´í„°**:
- [x] ê° ë¬¸í•­ì— `stage`, `panel`, `form_id`, `slot_position` ë°°ì •
- [x] ì´ˆê¸° ë‚œì´ë„ ì¶”ì • (ì „ë¬¸ê°€ íŒë‹¨ ë˜ëŠ” Rasch ëª¨ë¸)

#### **Week 7-10: íŒŒì¼ëŸ¿ í…ŒìŠ¤íŠ¸**

**ê³ ì •í˜• í¼ ì œì‘**:
- [x] 3ê°œ ë™í˜• í¼ (ê° 40ë¬¸í•­)
- [x] ë„ë©”ì¸ ê· í˜• ê²€ì¦

**ë°ì´í„° ìˆ˜ì§‘**:
- [x] 200-500ëª… í•™ìƒ ëª¨ì§‘
- [x] ì˜¨ë¼ì¸ ì„¤ë¬¸ ë˜ëŠ” í•™êµ í˜‘ë ¥

**IRT ìº˜ë¦¬ë¸Œë ˆì´ì…˜**:
- [x] GIRTHë¡œ 3PL MML ì¶”ì •
- [x] ë¶€ì í•© ë¬¸í•­ í”Œë˜ê·¸ (Point-biserial < 0.2)
- [x] ë¬¸í•­ êµì²´/ìˆ˜ì • (10-20%)

#### **Week 11-12: MST ì‹œìŠ¤í…œ êµ¬ì¶•**

**Backend API**:
```python
# fastapi_app/routers/mst.py

@router.post("/test/start")
async def start_test(student_id: int):
    # Stage 1 ë¬¸í•­ 8ê°œ ë¡œë“œ (ë™í˜• ë¡œí…Œì´ì…˜)
    form_id = student_id % 3
    items = load_stage1_items(form_id)

    # ì„¸ì…˜ ìƒì„±
    session = create_test_session(student_id, stage=1)

    return {
        "session_id": session.id,
        "stage": 1,
        "items": items
    }

@router.post("/test/submit-stage1")
async def submit_stage1(session_id: int, responses: List[Response]):
    # Î¸â‚ ì¶”ì •
    theta1, se1 = estimate_theta_eap(responses)

    # ë¼ìš°íŒ… ê²°ì •
    track = route_stage1(theta1)  # Low/Med/High

    # Stage 2 ë¬¸í•­ ë¡œë“œ
    form_id = get_session(session_id).student_id % 3
    items = load_stage2_items(track, form_id)

    # ì„¸ì…˜ ì—…ë°ì´íŠ¸
    update_session(session_id, {
        'theta_trace': [{'stage': 1, 'theta': theta1, 'se': se1}],
        'stage2_panel': f'stage2_{track}_form{form_id}'
    })

    return {
        "theta1": theta1,
        "se1": se1,
        "track": track,
        "stage": 2,
        "items": items
    }

# ... submit-stage2, complete ì—”ë“œí¬ì¸íŠ¸ ë™ì¼ íŒ¨í„´
```

**ë…¸ì¶œ ì œì–´**:
- [x] Randomesque ë¡œì§ êµ¬í˜„
- [x] `exposure_count` ì¦ê°€
- [x] `exposure_cap` ì´ˆê³¼ ì‹œ `status='retired'`

#### **Week 13-14: Lexile/AR ì¶”ì • ëª¨ë¸**

**í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘**:
- [x] CommonLit, Newsela ë“±ì—ì„œ 500+ í…ìŠ¤íŠ¸
- [x] Lexile ë¼ë²¨ í™•ë³´ (ê³µê°œ ë°ì´í„°ì…‹)

**ëª¨ë¸ í•™ìŠµ**:
- [x] Textstat, Wordfreq, spaCyë¡œ íŠ¹ì§• ì¶”ì¶œ
- [x] Gradient Boosting í•™ìŠµ (RÂ² â‰¥ 0.85)
- [x] ëª¨ë¸ ì €ì¥ (`models/lexile_estimator.pkl`)

**Lexile â†” AR ë³€í™˜**:
- [x] ë³€í™˜í‘œ êµ¬í˜„ (ì„ í˜• ë³´ê°„)

#### **Week 15-16: ë¦¬í¬íŠ¸ ì‹œìŠ¤í…œ**

**Backend**:
```python
@router.post("/test/complete")
async def complete_test(session_id: int, responses: List[Response]):
    # ìµœì¢… Î¸ ì¶”ì •
    all_responses = get_all_responses(session_id)
    all_items = get_all_items(session_id)
    final_theta, final_se = estimate_theta_eap(all_responses, all_items)

    # 10ë ˆë²¨ ë³€í™˜
    level = theta_to_level(final_theta)

    # Lexile/AR ì˜ˆì¸¡
    passages = get_reading_passages(session_id)
    avg_lexile = predict_lexile(passages)
    avg_ar = lexile_to_ar(avg_lexile)

    # ì–´íœ˜ ì‚¬ì´ì¦ˆ
    vocab_items = get_vocabulary_items(session_id)
    vocab_responses = get_vocabulary_responses(session_id)
    vocab_result = estimate_vocabulary_size(vocab_responses, vocab_items)

    # ë„ë©”ì¸ë³„ ë¶„ì„
    domain_analysis = analyze_domain_performance(all_responses, all_items)

    # ì„¸ì…˜ ì—…ë°ì´íŠ¸
    update_session(session_id, {
        'final_theta': final_theta,
        'final_se': final_se,
        'diagnostic_level': level,
        'estimated_lexile': avg_lexile,
        'estimated_ar': avg_ar,
        'vocabulary_size': vocab_result['total'],
        'vocab_confidence': vocab_result['confidence'],
        'grammar_score': domain_analysis['grammar']['score'],
        'vocabulary_score': domain_analysis['vocabulary']['score'],
        'reading_score': domain_analysis['reading']['score'],
        'completed_at': datetime.now()
    })

    return {
        "level": level,
        "description": LEVEL_DESCRIPTIONS[level],
        "lexile": avg_lexile,
        "ar": avg_ar,
        "vocabulary_size": vocab_result['total'],
        "domains": domain_analysis
    }
```

**Frontend (Next.js)**:
- [x] `<ReportScreen>` ì»´í¬ë„ŒíŠ¸
- [x] Rechartsë¡œ ë ˆì´ë” ì°¨íŠ¸
- [x] PDF ë‹¤ìš´ë¡œë“œ (react-pdf)

#### **Week 17-18: ë² íƒ€ í…ŒìŠ¤íŠ¸**

**í…ŒìŠ¤íŠ¸ ì‹¤ì‹œ**:
- [x] 100ëª… í•™ìƒ MST ì‹¤ì‹œ
- [x] SE ë¶„í¬ í™•ì¸ (ëª©í‘œ: â‰¥ 95%ê°€ SE â‰¤ 0.30)
- [x] ë…¸ì¶œë¥  ëª¨ë‹ˆí„°ë§ (ìµœëŒ€ ë…¸ì¶œ â‰¤ 30%)

**í”¼ë“œë°± ìˆ˜ì§‘**:
- [x] ì‚¬ìš©ì ê²½í—˜ ì„¤ë¬¸
- [x] UI/UX ê°œì„ 
- [x] ë²„ê·¸ ìˆ˜ì •

### 7.2 Phase 2: í™•ì¥ (6ê°œì›”)

#### **Month 4-5: ë¬¸í•­ í’€ í™•ì¥ (600ê°œ)**

- [ ] ê° íŒ¨ë„ ë™í˜• 6ì„¸íŠ¸ë¡œ í™•ì¥
- [ ] ë¬¸í•­ 300ê°œ ì¶”ê°€ ì œì‘
- [ ] ì¬ìº˜ë¦¬ë¸Œë ˆì´ì…˜ (GIRTH)

#### **Month 6-7: êµì‚¬ìš© ëŒ€ì‹œë³´ë“œ**

- [ ] í•™ê¸‰ ê´€ë¦¬ ê¸°ëŠ¥
- [ ] í•™ìƒë³„ ì§„ë‹¨ ê²°ê³¼ ì¡°íšŒ
- [ ] í•™ê¸‰ í‰ê· /ë¶„í¬ ì°¨íŠ¸
- [ ] ì—‘ì…€ ë‹¤ìš´ë¡œë“œ

#### **Month 8-9: í•™ìŠµ ì¶”ì²œ ì‹œìŠ¤í…œ**

- [ ] ë ˆë²¨ë³„ ì¶”ì²œ ë„ì„œ DB êµ¬ì¶•
- [ ] Open Library API ì—°ë™
- [ ] Lexile ê¸°ë°˜ í•„í„°ë§

#### **Month 10-12: ëª¨ë°”ì¼ ìµœì í™”**

- [ ] ë°˜ì‘í˜• ë””ìì¸ ê°œì„ 
- [ ] í„°ì¹˜ UI ìµœì í™”
- [ ] Progressive Web App (PWA)

---

## ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­

### 8.1 ì‘ë‹µ ì‹œê°„ (Response Time)

| ì—”ë“œí¬ì¸íŠ¸ | ëª©í‘œ | ìµœëŒ€ í—ˆìš© |
|-----------|------|----------|
| `POST /test/start` | < 500ms | 1s |
| `POST /test/submit-stage1` | < 1s | 2s |
| `POST /test/submit-stage2` | < 1s | 2s |
| `POST /test/complete` | < 3s | 5s |
| `GET /report/{session_id}` | < 200ms | 500ms |

**ìµœì í™” ì „ëµ**:
- [x] DB ì¸ë±ìŠ¤ í™œìš© (items, sessions)
- [x] EAP ì ë¶„ ìºì‹± (Quadrature points ë¯¸ë¦¬ ê³„ì‚°)
- [x] ML ëª¨ë¸ ë©”ëª¨ë¦¬ ë¡œë“œ (ì„œë²„ ì‹œì‘ ì‹œ)

### 8.2 ë™ì‹œ ì ‘ì† (Concurrency)

| ì§€í‘œ | ëª©í‘œ |
|------|------|
| ë™ì‹œ í…ŒìŠ¤íŠ¸ ì„¸ì…˜ | 100ëª… |
| ì´ˆë‹¹ ìš”ì²­ ì²˜ë¦¬ (RPS) | 50 req/s |
| DB ì»¤ë„¥ì…˜ í’€ | 20 connections |

**ìŠ¤ì¼€ì¼ë§ ê³„íš**:
- **Horizontal Scaling**: Renderì—ì„œ ì¸ìŠ¤í„´ìŠ¤ ì¶”ê°€
- **DB Optimization**: Supabase Connection Pooler ì‚¬ìš©
- **Caching**: Redis (Phase 2)

### 8.3 ì •í™•ë„ (Accuracy)

| ì§€í‘œ | ëª©í‘œ |
|------|------|
| IRT SE | â‰¤ 0.30 (95% í•™ìƒ) |
| Lexile ëª¨ë¸ RÂ² | â‰¥ 0.85 |
| Lexile ëª¨ë¸ MAE | < 70L |
| ì–´íœ˜ ì‚¬ì´ì¦ˆ ì‹ ë¢°ë„ | High (80% í•™ìƒ) |

**ê²€ì¦ ë°©ë²•**:
- íŒŒì¼ëŸ¿ ë°ì´í„°ë¡œ SE ë¶„í¬ í™•ì¸
- Test setìœ¼ë¡œ Lexile ëª¨ë¸ í‰ê°€
- ê°€ì§œì–´ ì˜¤íƒë¥  ëª¨ë‹ˆí„°ë§ (< 10%)

---

## ë³´ì•ˆ ë° ê°œì¸ì •ë³´ë³´í˜¸

### 9.1 ë°ì´í„° ë³´í˜¸

**ì•”í˜¸í™”**:
- [x] HTTPS only (Netlify/Render ìë™)
- [x] DB ì•”í˜¸í™” (Supabase ê¸°ë³¸ ì œê³µ)
- [x] ë¹„ë°€ë²ˆí˜¸ í•´ì‹± (bcrypt, salt rounds=12)

**ì ‘ê·¼ ì œì–´**:
- [x] JWT í† í° ì¸ì¦ (ë§Œë£Œ 24ì‹œê°„)
- [x] Role-based Access Control (í•™ìƒ/êµì‚¬/ê´€ë¦¬ì)
- [x] ì„¸ì…˜ë³„ student_id ê²€ì¦

### 9.2 ê°œì¸ì •ë³´ ì²˜ë¦¬

**ìˆ˜ì§‘ ì •ë³´**:
- í•„ìˆ˜: ì´ë©”ì¼, ì´ë¦„
- ì„ íƒ: í•™ë…„, í•™êµ

**ë³´ê´€ ê¸°ê°„**:
- í…ŒìŠ¤íŠ¸ ê²°ê³¼: 2ë…„
- ì‘ë‹µ ë¡œê·¸: 1ë…„ (í†µê³„ ëª©ì )
- ë¹„í™œì„± ê³„ì •: 6ê°œì›” í›„ ì‚­ì œ ì•ˆë‚´

**GDPR/ê°œì¸ì •ë³´ë³´í˜¸ë²• ì¤€ìˆ˜**:
- [x] ê°œì¸ì •ë³´ ì²˜ë¦¬ë°©ì¹¨ ëª…ì‹œ
- [x] ë°ì´í„° ì‚­ì œ ìš”ì²­ ê¸°ëŠ¥ (`DELETE /api/user/{id}`)
- [x] ë°ì´í„° ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ (JSON)

### 9.3 ë¬¸í•­ ë³´ì•ˆ

**ë…¸ì¶œ ì œì–´**:
- [x] Randomesque ë¡œì§ (ìŠ¬ë¡¯ë‹¹ 2-3ê°œ í›„ë³´)
- [x] ë™í˜• ë¡œí…Œì´ì…˜ (3ê°œ â†’ 6ê°œ í¼)
- [x] `exposure_cap` ì´ˆê³¼ ì‹œ ë¬¸í•­ ì€í‡´
- [x] ì¬ì‘ì‹œ ê°„ê²© ì œí•œ (2ì£¼)

**ë¬¸í•­ ìœ ì¶œ ë°©ì§€**:
- [x] API ì‘ë‹µì— `correct_answer` ì œì™¸ (í…ŒìŠ¤íŠ¸ ì¤‘)
- [x] ì™„ë£Œ í›„ì—ë§Œ ì •ì˜¤ë‹µ ê³µê°œ
- [x] ê´€ë¦¬ì ì „ìš© ì—”ë“œí¬ì¸íŠ¸ ë¶„ë¦¬

---

## í…ŒìŠ¤íŠ¸ ê³„íš

### 10.1 ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (Unit Tests)

**Backend (Pytest)**:
```python
# tests/test_irt.py

def test_eap_estimate():
    """EAP Î¸ ì¶”ì • ì •í™•ë„ í…ŒìŠ¤íŠ¸"""
    # Given: ì•Œë ¤ì§„ Î¸=0.5ì¸ í•™ìƒì˜ ì´ë¡ ì  ì‘ë‹µ íŒ¨í„´
    items = [
        {'a': 1.0, 'b': 0.0, 'c': 0.25},
        {'a': 1.5, 'b': 0.5, 'c': 0.25},
        # ... 8ê°œ
    ]
    # Î¸=0.5ì¼ ë•Œ ê¸°ëŒ€ ì •ë‹µë¥ ë¡œ ì‘ë‹µ ìƒì„±
    responses = generate_responses(theta=0.5, items=items)

    # When: EAP ì¶”ì •
    estimated_theta, se = eap_estimate(responses, items)

    # Then: Â±0.1 ì˜¤ì°¨ ë‚´
    assert abs(estimated_theta - 0.5) < 0.1
    assert se < 0.8  # Stage 1 ìˆ˜ì¤€

def test_route_stage1():
    """Stage 1 ë¼ìš°íŒ… ë¡œì§ í…ŒìŠ¤íŠ¸"""
    assert route_stage1(-0.6) == 'Low'
    assert route_stage1(0.0) == 'Med'
    assert route_stage1(0.8) == 'High'

# tests/test_lexile.py

def test_lexile_model():
    """Lexile ëª¨ë¸ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸"""
    # Given: ì•Œë ¤ì§„ Lexile ì ìˆ˜ì˜ í…ìŠ¤íŠ¸
    passage = "Sample passage with known Lexile 800L..."

    # When: íŠ¹ì§• ì¶”ì¶œ + ì˜ˆì¸¡
    features = extract_text_features(passage)
    predicted_lexile = lexile_model.predict([features])[0]

    # Then: Â±100L ì˜¤ì°¨ ë‚´
    assert abs(predicted_lexile - 800) < 100
```

**Frontend (Jest + React Testing Library)**:
```typescript
// __tests__/ItemDisplay.test.tsx

test('renders item with 4 options', () => {
  const item = {
    stem: 'What is 2+2?',
    options: ['A) 3', 'B) 4', 'C) 5', 'D) 6']
  };

  render(<ItemDisplay item={item} onSelect={jest.fn()} />);

  expect(screen.getByText('What is 2+2?')).toBeInTheDocument();
  expect(screen.getByText('A) 3')).toBeInTheDocument();
  // ... 4ê°œ ì˜µì…˜ ëª¨ë‘ í™•ì¸
});

test('calls onSelect when option clicked', () => {
  const mockOnSelect = jest.fn();
  render(<ItemDisplay item={mockItem} onSelect={mockOnSelect} />);

  fireEvent.click(screen.getByText('B) 4'));

  expect(mockOnSelect).toHaveBeenCalledWith('B');
});
```

### 10.2 í†µí•© í…ŒìŠ¤íŠ¸ (Integration Tests)

**ì‹œë‚˜ë¦¬ì˜¤: ì „ì²´ MST í”Œë¡œìš°**:
```python
# tests/integration/test_mst_flow.py

async def test_full_mst_flow():
    """ì „ì²´ MST í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸"""
    client = TestClient(app)

    # 1. í…ŒìŠ¤íŠ¸ ì‹œì‘
    response = client.post('/api/test/start', json={'student_id': 999})
    assert response.status_code == 200
    data = response.json()
    session_id = data['session_id']
    stage1_items = data['items']
    assert len(stage1_items) == 8

    # 2. Stage 1 ì œì¶œ (ì¤‘ê°„ ë‚œì´ë„ ì‘ë‹µ íŒ¨í„´)
    stage1_responses = [
        {'item_id': item['id'], 'answer': 'A', 'response_time_ms': 10000}
        for item in stage1_items[:5]  # 5ê°œ ë§ìŒ
    ] + [
        {'item_id': item['id'], 'answer': 'Z', 'response_time_ms': 10000}
        for item in stage1_items[5:]  # 3ê°œ í‹€ë¦¼
    ]

    response = client.post('/api/test/submit-stage1', json={
        'session_id': session_id,
        'responses': stage1_responses
    })
    assert response.status_code == 200
    data = response.json()
    assert data['track'] == 'Med'  # 5/8 â†’ Î¸ â‰ˆ 0.0 â†’ Med
    stage2_items = data['items']
    assert len(stage2_items) == 16

    # 3. Stage 2 ì œì¶œ (ìœ ì‚¬ íŒ¨í„´)
    # ... (ìƒëµ)

    # 4. Stage 3 ì œì¶œ
    # ... (ìƒëµ)

    # 5. ì™„ë£Œ + ë¦¬í¬íŠ¸ í™•ì¸
    response = client.post('/api/test/complete', json={
        'session_id': session_id,
        'responses': stage3_responses
    })
    assert response.status_code == 200
    report = response.json()

    assert 1 <= report['level'] <= 10
    assert 0 <= report['lexile'] <= 1700
    assert 0.5 <= report['ar'] <= 12.9
    assert report['vocabulary_size'] > 0
    assert 'grammar' in report['domains']
```

### 10.3 ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (Performance Tests)

**Locust (ë¶€í•˜ í…ŒìŠ¤íŠ¸)**:
```python
# tests/performance/locustfile.py

from locust import HttpUser, task, between

class TestUser(HttpUser):
    wait_time = between(1, 3)

    def on_start(self):
        """í…ŒìŠ¤íŠ¸ ì‹œì‘"""
        response = self.client.post('/api/test/start', json={
            'student_id': self.environment.runner.user_count
        })
        self.session_id = response.json()['session_id']

    @task
    def submit_stage1(self):
        """Stage 1 ì œì¶œ"""
        # ... (ì‘ë‹µ ìƒì„±)
        with self.client.post('/api/test/submit-stage1', json={...},
                               catch_response=True) as response:
            if response.elapsed.total_seconds() > 2:
                response.failure('Stage 1 took > 2s')

    # ... submit_stage2, complete

# ì‹¤í–‰:
# locust -f locustfile.py --users 100 --spawn-rate 10 --host https://api.example.com
```

**ëª©í‘œ**:
- 100ëª… ë™ì‹œ ì ‘ì† ì‹œ í‰ê·  ì‘ë‹µ ì‹œê°„ < 1s
- P95 ì‘ë‹µ ì‹œê°„ < 2s

### 10.4 ì‚¬ìš©ì ìˆ˜ìš© í…ŒìŠ¤íŠ¸ (UAT)

**ë² íƒ€ í…ŒìŠ¤íŠ¸ ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] 100ëª… í•™ìƒ ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ
- [ ] SE â‰¥ 95%ê°€ â‰¤ 0.30 í™•ì¸
- [ ] ë…¸ì¶œë¥  ìµœëŒ€ 30% ë¯¸ë§Œ í™•ì¸
- [ ] UI/UX ì„¤ë¬¸ NPS â‰¥ 50
- [ ] í¬ë¦¬í‹°ì»¬ ë²„ê·¸ 0ê±´

---

## ë¶€ë¡ (Appendix)

### A. ìš©ì–´ ì •ë¦¬ (Glossary)

| ìš©ì–´ | ì„¤ëª… |
|------|------|
| **MST** | Multistage Testing, ë‹¤ë‹¨ê³„ ì ì‘í˜• ê²€ì‚¬ |
| **IRT** | Item Response Theory, ë¬¸í•­ë°˜ì‘ì´ë¡  |
| **3PL** | 3-Parameter Logistic Model (a, b, c) |
| **EAP** | Expected A Posteriori, ì‚¬í›„ê¸°ëŒ“ê°’ ì¶”ì • |
| **SE** | Standard Error, í‘œì¤€ì˜¤ì°¨ (ì¸¡ì • ì •í™•ë„) |
| **Î¸ (theta)** | ëŠ¥ë ¥ì¹˜ íŒŒë¼ë¯¸í„° (Ability parameter) |
| **Lexile** | MetaMetricsì˜ í…ìŠ¤íŠ¸ ë‚œì´ë„ ì§€ìˆ˜ (0-1700L) |
| **AR** | Accelerated Reader, Renaissance Learningì˜ í•™ë…„ ìˆ˜ì¤€ ì§€ìˆ˜ (K-12) |
| **VST** | Vocabulary Size Test, Paul Nationì˜ ì–´íœ˜ëŸ‰ ì¸¡ì • ë°©ì‹ |
| **Randomesque** | ì¤€ë¬´ì‘ìœ„ ë¬¸í•­ ì„ íƒ (ë…¸ì¶œ ì œì–´ ê¸°ë²•) |
| **Bookmark** | IRT Î¸ë¥¼ ì ˆëŒ€ ê¸°ì¤€ ë ˆë²¨ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²• |

### B. ì°¸ê³  ë¬¸í—Œ (References)

**IRT & MST**:
1. Hambleton, R. K., & Swaminathan, H. (1985). *Item Response Theory: Principles and Applications*. Springer.
2. Yan, D., von Davier, A. A., & Lewis, C. (2014). *Computerized Multistage Testing: Theory and Applications*. CRC Press.

**Lexile & Readability**:
3. Stenner, A. J., et al. (2006). "The Lexile Framework for Reading." MetaMetrics.
4. Dale, E., & Chall, J. S. (1948). "A Formula for Predicting Readability." *Educational Research Bulletin*.

**Vocabulary Size**:
5. Nation, I. S. P., & Beglar, D. (2007). "A vocabulary size test." *The Language Teacher*, 31(7), 9-13.
6. Schmitt, N., et al. (2001). "Developing and exploring the behaviour of two new versions of the Vocabulary Levels Test." *Language Testing*, 18(1), 55-88.

**Python Libraries**:
7. GIRTH: https://github.com/eribean/girth
8. Textstat: https://github.com/shivam5992/textstat
9. Wordfreq: https://github.com/rspeer/wordfreq

### C. ë°ì´í„° ì†ŒìŠ¤ (Data Sources)

**ë¬¸í•­ ì œì‘ ì°¸ê³ **:
- Cambridge English Corpus
- TOEFL/TOEIC ê³µê°œ ìƒ˜í”Œ
- CommonLit (https://www.commonlit.org/)
- Project Gutenberg (ê³µê°œ ë„ë©”ì¸ í…ìŠ¤íŠ¸)

**Lexile í•™ìŠµ ë°ì´í„°**:
- CommonLit + Lexile labels
- Newsela corpus
- Find a Book (Lexile DB)

**ì–´íœ˜ ë¹ˆë„**:
- BNC/COCA word lists (Mark Davies)
- Google Books Ngrams

---

**ë¬¸ì„œ ë²„ì „**: 1.0.0
**ìµœì¢… ìˆ˜ì •**: 2025-01-20
**ì‘ì„±ì**: ê°œë°œíŒ€ + Claude Code
**ìŠ¹ì¸**: (ê²€í†  ëŒ€ê¸°)

# AI-Powered Adaptive CAT System Design
**ì˜ì–´ ëŠ¥ë ¥ í‰ê°€ AI ì—°ë™í˜• ì ì‘í˜• ì»´í“¨í„° ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ**

---

## ğŸ“‹ Executive Summary

### Current System (ê¸°ì¡´ ì‹œìŠ¤í…œ)
- **MST (Multi-Stage Testing)**: 1â†’3â†’3 êµ¬ì¡°ì˜ ê³ ì •ëœ ë‹¨ê³„
- **IRT 3PL ëª¨ë¸**: ë¬¸í•­ ë‚œì´ë„ ê¸°ë°˜ ëŠ¥ë ¥ ì¶”ì •
- **Static Item Pool**: 600ê°œ ê³ ì • ë¬¸í•­ (í˜„ì¬ 237ê°œ)
- **Manual Quality Control**: ìˆ˜ë™ ë¬¸í•­ ê²€í†  ë° ìˆ˜ì •

### Proposed AI-Enhanced System (AI ê°•í™” ì œì•ˆ ì‹œìŠ¤í…œ)
- **Dynamic Item Generation**: AI ì‹¤ì‹œê°„ ë¬¸í•­ ìƒì„±
- **Adaptive Difficulty**: ì‹¤ì‹œê°„ ë‚œì´ë„ ì¡°ì •
- **Automated QA**: AI ê¸°ë°˜ ë¬¸í•­ í’ˆì§ˆ ê²€ì¦
- **Continuous Learning**: í•™ìŠµì ë°˜ì‘ ê¸°ë°˜ ëª¨ë¸ ì—…ë°ì´íŠ¸

---

## ğŸ¯ Core Objectives

### 1. Quality Improvement (í’ˆì§ˆ ê°œì„ )
- âœ… **No more duplicates**: AI generates unique items
- âœ… **Grammar accuracy**: AI validates grammatical correctness
- âœ… **Context relevance**: Ensures meaningful, real-world contexts
- âœ… **Difficulty precision**: IRT-calibrated item parameters

### 2. Scalability (í™•ì¥ì„±)
- ğŸš€ **Unlimited item pool**: Generate items on-demand
- ğŸš€ **Multi-domain coverage**: Grammar, vocabulary, reading
- ğŸš€ **Level diversity**: A1-C2 CEFR levels
- ğŸš€ **Content freshness**: Regular item rotation

### 3. Personalization (ê°œì¸í™”)
- ğŸ¯ **Learner profiling**: Track strengths/weaknesses
- ğŸ¯ **Adaptive pathways**: Customize test flow
- ğŸ¯ **Targeted feedback**: Specific skill recommendations
- ğŸ¯ **Progress tracking**: Longitudinal ability monitoring

---

## ğŸ—ï¸ System Architecture

### Component Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Frontend (React)                        â”‚
â”‚  - Test Interface                                           â”‚
â”‚  - Results Dashboard                                        â”‚
â”‚  - Admin Panel                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Node.js Backend (Proxy Layer)                  â”‚
â”‚  - Authentication                                           â”‚
â”‚  - Session Management                                       â”‚
â”‚  - Request Routing                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                     â”‚
â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Python       â”‚  â”‚  AI Service      â”‚
â”‚  FastAPI      â”‚  â”‚  (Claude API)    â”‚
â”‚  - IRT Engine â”‚  â”‚  - Item Gen      â”‚
â”‚  - MST Logic  â”‚  â”‚  - QA Check      â”‚
â”‚  - Scoring    â”‚  â”‚  - Analysis      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL        â”‚
â”‚   (Supabase)        â”‚
â”‚   - Items           â”‚
â”‚   - Sessions        â”‚
â”‚   - Responses       â”‚
â”‚   - AI Metadata     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¤– AI Integration Points

### 1. Intelligent Item Generation

#### Input Parameters
```json
{
  "domain": "vocabulary|grammar|reading",
  "difficulty": -3.0 to +3.0,  // IRT b parameter
  "discrimination": 0.5 to 2.5, // IRT a parameter
  "skill_tag": "present_perfect|conditionals|inference",
  "text_type": "expository|narrative|argumentative",
  "cefr_level": "A1|A2|B1|B2|C1|C2",
  "constraints": {
    "word_count": 50-500,
    "avoid_topics": ["politics", "religion"],
    "required_features": ["real_world_context"]
  }
}
```

#### AI Prompt Template
```
You are an expert English language test item writer following IRT 3PL principles.

Generate a {domain} test item with these specifications:
- CEFR Level: {cefr_level}
- Skill: {skill_tag}
- Difficulty: {difficulty_description}
- Discrimination: {discrimination_level}

Requirements:
1. Item must have exactly 4 options (A, B, C, D)
2. Only ONE correct answer
3. Distractors must be plausible but clearly incorrect
4. Use natural, authentic language
5. Avoid cultural bias or sensitive topics
6. For reading items, include a passage (50-200 words)

Output format (JSON):
{
  "stem": "question text",
  "passage": "reading passage (if applicable)",
  "options": {"A": "...", "B": "...", "C": "...", "D": "..."},
  "correct_answer": "A",
  "rationale": "why this tests the target skill",
  "estimated_difficulty": -1.5,
  "estimated_discrimination": 1.2
}
```

### 2. Automated Quality Assurance

#### QA Checks
```python
class AIItemQA:
    def validate_item(self, item):
        checks = [
            self.check_grammar(),           # Grammar correctness
            self.check_uniqueness(),        # No duplicates
            self.check_difficulty(),        # Matches target
            self.check_distractors(),       # Quality of wrong answers
            self.check_bias(),              # Cultural/gender bias
            self.check_answer_key(),        # Only one correct answer
        ]
        return all(checks)

    def check_grammar(self, text):
        """Use Claude API to verify grammatical correctness"""
        prompt = f"Is this sentence grammatically correct? {text}"
        # Claude API call

    def check_uniqueness(self, stem):
        """Check against existing items in database"""
        # Cosine similarity with existing items
        # Reject if similarity > 0.85

    def check_difficulty(self, item, target_difficulty):
        """Estimate difficulty and compare to target"""
        # Use AI to estimate difficulty
        # Compare with target Â± 0.5 range
```

### 3. Dynamic Difficulty Adjustment

#### Real-Time Calibration
```python
class AdaptiveItemSelector:
    def select_next_item(self, theta_estimate, se, responses):
        # Traditional: Select from static pool
        # AI-Enhanced: Generate if no suitable item

        candidates = self.query_item_pool(
            difficulty_range=(theta_estimate - 1, theta_estimate + 1),
            min_discrimination=1.0
        )

        if len(candidates) < 3:
            # Generate new item on-the-fly
            new_item = self.ai_generate_item(
                target_difficulty=theta_estimate,
                domain=self.next_domain(),
                avoid_skills=self.tested_skills
            )

            # Quick QA
            if self.ai_qa.validate_item(new_item):
                candidates.append(new_item)

        # Fisher Information-based selection
        selected = max(candidates, key=lambda i: self.fisher_info(i, theta_estimate))
        return selected
```

### 4. Intelligent Feedback Generation

#### Personalized Reports
```python
class AIFeedbackGenerator:
    def generate_report(self, session_data):
        prompt = f"""
        Analyze this test performance and generate actionable feedback:

        - Final ability: {session_data['theta']} (SE: {session_data['se']})
        - Vocabulary: {session_data['vocab_score']}/14 correct
        - Grammar: {session_data['grammar_score']}/13 correct
        - Reading: {session_data['reading_score']}/13 correct

        Weak areas: {session_data['weak_skills']}
        Strong areas: {session_data['strong_skills']}

        Provide:
        1. Overall proficiency level (CEFR)
        2. Domain-specific strengths/weaknesses
        3. 3-5 specific study recommendations
        4. Suggested resources (books, exercises)
        """

        return claude_api_call(prompt)
```

---

## ğŸ“Š Data Flow

### Test Session Lifecycle

```
1. Start Test
   â”œâ”€> AI generates initial routing module (8 items)
   â””â”€> IRT estimates initial Î¸

2. Stage 1 (Routing)
   â”œâ”€> Select/generate items near Î¸=0
   â”œâ”€> Record responses
   â””â”€> Update Î¸ estimate

3. Stage 2 Routing
   â”œâ”€> AI generates panel-specific items
   â”‚   â”œâ”€> Low: Î¸ < -1.0
   â”‚   â”œâ”€> Medium: -1.0 â‰¤ Î¸ â‰¤ 1.0
   â”‚   â””â”€> High: Î¸ > 1.0
   â””â”€> Record 16 responses

4. Stage 3 Routing
   â”œâ”€> AI generates subtrack items
   â””â”€> Record final 16 responses

5. Finalize
   â”œâ”€> Calculate final Î¸ and SE
   â”œâ”€> Map to CEFR/Lexile/AR
   â”œâ”€> AI generates feedback report
   â””â”€> Store all data for model training
```

---

## ğŸ› ï¸ Implementation Plan

### Phase 1: AI Item Generation Service (Week 1-2)

**Deliverables:**
- [ ] Claude API integration module
- [ ] Item generation prompt templates
- [ ] JSON schema validation
- [ ] Initial QA pipeline

**Files to Create:**
- `backend/app/ai_service/item_generator.py`
- `backend/app/ai_service/qa_validator.py`
- `backend/app/ai_service/prompts.py`

### Phase 2: Dynamic Item Pool (Week 3-4)

**Deliverables:**
- [ ] AI-generated item caching
- [ ] Real-time item generation endpoint
- [ ] Quality scoring system
- [ ] Item metadata tracking

**Database Changes:**
```sql
-- Add AI metadata to items table
ALTER TABLE items ADD COLUMN ai_generated BOOLEAN DEFAULT FALSE;
ALTER TABLE items ADD COLUMN generation_model VARCHAR(50);
ALTER TABLE items ADD COLUMN qa_score FLOAT;
ALTER TABLE items ADD COLUMN generation_prompt TEXT;
```

### Phase 3: Adaptive Test Engine (Week 5-6)

**Deliverables:**
- [ ] Hybrid static/dynamic item selection
- [ ] Real-time difficulty adjustment
- [ ] Exposure control for AI items
- [ ] Performance monitoring

### Phase 4: Admin Dashboard (Week 7-8)

**Deliverables:**
- [ ] AI item generation UI
- [ ] Quality review interface
- [ ] Item analytics dashboard
- [ ] Manual override controls

---

## ğŸ‘¤ Admin Panel Features

### 1. AI Item Generator

**UI Components:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Generate New Items                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Domain:     [Vocabulary â–¼]           â”‚
â”‚ CEFR Level: [B1 â–¼]                   â”‚
â”‚ Skill:      [Collocations â–¼]         â”‚
â”‚ Quantity:   [10        ]             â”‚
â”‚                                      â”‚
â”‚ Advanced Options â–¼                   â”‚
â”‚  â˜‘ Auto-validate grammar             â”‚
â”‚  â˜‘ Check for duplicates              â”‚
â”‚  â˜‘ Estimate IRT parameters           â”‚
â”‚  â˜ Generate with passages            â”‚
â”‚                                      â”‚
â”‚ [Generate Items]  [Preview First]   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Quality Review Dashboard

**Features:**
- Review AI-generated items before activation
- Approve/reject/edit functionality
- Batch operations
- Quality score visualization

### 3. Item Analytics

**Metrics:**
- Exposure rates
- p-values (difficulty)
- Point-biserial correlations
- IRT parameter drift
- AI vs human-written performance

### 4. Test Configuration

**Settings:**
- Enable/disable AI generation
- Set quality thresholds
- Configure item rotation
- Manage item pools

---

## ğŸ”’ Quality Assurance

### Multi-Layer Validation

1. **Pre-Generation**
   - Validate input parameters
   - Check domain constraints
   - Verify target difficulty range

2. **Post-Generation**
   - Grammar check (AI + rule-based)
   - Duplicate detection (cosine similarity)
   - Answer key validation
   - Distractor quality analysis

3. **Pre-Deployment**
   - Admin review (optional)
   - Pilot testing (optional)
   - IRT calibration (field testing)

4. **Post-Deployment**
   - Monitor performance metrics
   - Flag anomalies
   - Continuous recalibration

---

## ğŸ“ˆ Success Metrics

### Item Quality
- Grammar error rate < 1%
- Duplicate rate < 0.1%
- Distractor effectiveness > 10% selection rate
- Point-biserial > 0.15

### Test Efficiency
- Standard error < 0.4 after 40 items
- Test-retest reliability > 0.85
- Item exposure balanced (SD < 5%)

### User Experience
- Test completion rate > 95%
- Average completion time: 30-40 minutes
- User satisfaction > 4.0/5.0

---

## ğŸš€ Future Enhancements

### Short-term (3-6 months)
- Multi-modal items (audio, images)
- Automated passage generation
- Real-time cheating detection
- Multi-language support (Korean, Spanish)

### Long-term (6-12 months)
- Speech assessment integration
- Writing assessment with AI scoring
- Personalized learning pathways
- Teacher dashboard with class analytics

---

## ğŸ’¡ Technical Considerations

### API Rate Limits
- Claude API: 100 requests/minute
- Cache frequently used items
- Pre-generate item pools during low-traffic hours

### Cost Optimization
- Batch generation requests
- Reuse similar items across panels
- Implement smart caching strategy
- Estimated cost: $0.10-0.30 per test

### Performance
- Target latency: <2s per item generation
- Database query optimization
- Redis caching for hot items
- CDN for static assets

---

## ğŸ“š References

### Academic Foundations
- IRT 3PL Model (Birnbaum, 1968)
- MST Design (Luecht & Nungester, 1998)
- CAT Algorithms (van der Linden & Glas, 2000)

### Implementation Guides
- Claude API Documentation
- Supabase PostgreSQL Best Practices
- FastAPI Async Patterns

---

**Document Version:** 1.0
**Last Updated:** 2025-01-27
**Author:** AI Architecture Team
**Status:** ğŸŸ¢ Approved for Implementation

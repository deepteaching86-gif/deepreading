# ì‹œì„  ì¶”ì  ì‹œìŠ¤í…œ ê°œë°œ ë¡œê·¸

> **í”„ë¡œì íŠ¸**: ë¬¸í•´ë ¥ ì§„ë‹¨ í‰ê°€ ì‹œìŠ¤í…œ - ì›¹ìº  ê¸°ë°˜ ì‹œì„  ì¶”ì 
> **ê¸°ê°„**: 2025ë…„ 1ì›” ~ ì§„í–‰ ì¤‘
> **ê¸°ìˆ  ìŠ¤íƒ**: React + TypeScript, MediaPipe Tasks Vision, Kalman Filter

---

## ğŸ“‹ ëª©ì°¨

1. [í˜„ì¬ ìƒíƒœ (Status)](#í˜„ì¬-ìƒíƒœ-status)
2. [í•´ê²°ëœ ë¬¸ì œë“¤ (Resolved Issues)](#í•´ê²°ëœ-ë¬¸ì œë“¤-resolved-issues)
3. [ì§„í–‰ ì¤‘ì¸ ë¬¸ì œ (Current Issues)](#ì§„í–‰-ì¤‘ì¸-ë¬¸ì œ-current-issues)
4. [í–¥í›„ ì‘ì—… ê³„íš (Roadmap)](#í–¥í›„-ì‘ì—…-ê³„íš-roadmap)
5. [ê¸°ìˆ  ìŠ¤íƒ ìƒì„¸ (Tech Stack)](#ê¸°ìˆ -ìŠ¤íƒ-ìƒì„¸-tech-stack)
6. [í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ (Core Algorithms)](#í•µì‹¬-ì•Œê³ ë¦¬ì¦˜-core-algorithms)
7. [ì°¸ê³  ìë£Œ (References)](#ì°¸ê³ -ìë£Œ-references)

---

## í˜„ì¬ ìƒíƒœ (Status)

### âœ… **ì‘ë™í•˜ëŠ” ê¸°ëŠ¥**

- âœ… **Camera Check Stage**: ì–¼êµ´ ì¸ì‹ ë° ì¤‘ì•™ ì •ë ¬ í™•ì¸
- âœ… **MediaPipe í†µí•©**: 478 facial landmarks + 5 iris landmarks per eye
- âœ… **Gaze Estimation**: 2D iris offset + head pose (yaw/pitch) ê¸°ë°˜ ì‹œì„  ì¶”ì •
- âœ… **Kalman Filtering**: ì‹œì„  ì¢Œí‘œ ë…¸ì´ì¦ˆ ê°ì†Œ
- âœ… **Video Stream Persistence**: Stage ì „í™˜ ì‹œ ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¼ ìœ ì§€

### âš ï¸ **ë¶€ë¶„ ì‘ë™ / ê°œì„  í•„ìš”**

- âš ï¸ **9-Point Calibration**: êµ¬í˜„ë˜ì–´ ìˆìœ¼ë‚˜ ì‹œì„  ê°ì§€ ì•ˆì •ì„± ë¬¸ì œë¡œ í…ŒìŠ¤íŠ¸ ì¤‘
- âš ï¸ **Vertical Tracking Sensitivity**: ìˆ˜ì§ ë°©í–¥ í™ì±„ ì›€ì§ì„ ê°ì§€ ì•½í•¨ (ê°œì„  ì‹œë„ ì¤‘)
- âš ï¸ **Gaze Marker Display**: Calibration ë‹¨ê³„ì—ì„œ ì‹œì„  ë§ˆì»¤ í‘œì‹œ ë¶ˆì•ˆì •

### âŒ **ì•Œë ¤ì§„ ë¬¸ì œ**

- âŒ **Stage Transition Video Loss**: Calibration ì§„ì… ì‹œ video `readyState: 0` ë¬´í•œ ë£¨í”„
  - **ìµœê·¼ ìˆ˜ì • (Commit c255a577)**: Detection loop restart ë°©ì§€ - í…ŒìŠ¤íŠ¸ ëŒ€ê¸° ì¤‘

---

## í•´ê²°ëœ ë¬¸ì œë“¤ (Resolved Issues)

### 1ï¸âƒ£ **Video Stream Lost During Calibration Stage Transition** (Commit 6e4507f6, 2a880658)

**ë¬¸ì œ**:
- `camera_check` â†’ `calibration` stage ì „í™˜ ì‹œ video elementì˜ `srcObject`ê°€ nullë¡œ ì´ˆê¸°í™”
- ë¬´í•œ `â³ Video not ready: {readyState: 0, width: 0, height: 0}` ë¡œê·¸

**ì›ì¸**:
```typescript
// React ì¡°ê±´ë¶€ ë Œë”ë§ìœ¼ë¡œ ìƒˆë¡œìš´ DOM element ìƒì„±
{stage === 'camera_check' && <video ref={videoRef} />}
{stage === 'calibration' && <video ref={videoRef} />}
```

**í•´ê²°ì±… (Commit 6e4507f6)**:
```typescript
// renderVideoCanvas() í•¨ìˆ˜ë¡œ ë™ì¼í•œ JSX êµ¬ì¡° ê³µìœ 
const renderVideoCanvas = () => (
  <>
    <video ref={videoRef} className={stage === 'camera_check' ? "..." : "hidden"} />
    <canvas ref={canvasRef} className={stage === 'camera_check' ? "..." : "hidden"} />
  </>
);

// ëª¨ë“  stageì—ì„œ ë™ì¼í•œ í•¨ìˆ˜ í˜¸ì¶œ
{renderVideoCanvas()}
```

**ì¶”ê°€ ìˆ˜ì • (Commit 2a880658)**:
```typescript
// style prop ë³€ê²½ë„ re-render ìœ ë°œ! â†’ í•­ìƒ ë™ì¼í•˜ê²Œ ìœ ì§€
style={{ transform: 'scaleX(-1)' }} // ALWAYS set - prevents re-render
```

---

### 2ï¸âƒ£ **Detection Loop Restart During Stage Transitions** (Commit c255a577) ğŸ”¥

**ë¬¸ì œ**:
- Stage ì „í™˜ ì‹œ `detectAndEstimateGaze` í•¨ìˆ˜ê°€ ì¬ìƒì„±ë˜ë©´ì„œ useEffect cleanup/re-run
- Detection loopê°€ ì¤‘ë‹¨ë˜ê³  ì¬ì‹œì‘ë˜ë©´ì„œ video stream ì†ì‹¤

**ì›ì¸**:
```typescript
// detectAndEstimateGazeê°€ dependency arrayì— í¬í•¨
useEffect(() => {
  if (isTracking && enabled) {
    detectAndEstimateGaze();
  }
  return () => cancelAnimationFrame(animationFrameRef.current);
}, [isTracking, enabled, detectAndEstimateGaze]); // â† ë¬¸ì œ!
```

**ì‹œë‚˜ë¦¬ì˜¤**:
1. Stage ë³€ê²½ â†’ callback í•¨ìˆ˜ ì¬ìƒì„± (`onFacePosition`, `onRawGazeData` etc.)
2. `detectAndEstimateGaze` dependency ë³€ê²½ â†’ í•¨ìˆ˜ ì¬ìƒì„±
3. useEffect cleanup â†’ `cancelAnimationFrame()` â†’ loop ì¤‘ë‹¨
4. useEffect re-run â†’ loop ì¬ì‹œì‘ â†’ í•˜ì§€ë§Œ videoRefê°€ React reconciliation ì¤‘!
5. Result: `readyState: 0`

**í•´ê²°ì±… (Commit c255a577)**:
```typescript
// detectAndEstimateGazeëŠ” self-sustaining loopì´ë¯€ë¡œ dependencyì—ì„œ ì œê±°!
useEffect(() => {
  if (isTracking && enabled) {
    detectAndEstimateGaze();
  }
  return () => cancelAnimationFrame(animationFrameRef.current);
}, [isTracking, enabled]); // Removed detectAndEstimateGaze!
```

**í•µì‹¬ ì´í•´**:
- `detectAndEstimateGaze()`ëŠ” ìê¸° ìì‹ ì„ ì¬ê·€ í˜¸ì¶œí•˜ëŠ” self-sustaining loop
- í•œ ë²ˆ ì‹œì‘í•˜ë©´ `requestAnimationFrame`ìœ¼ë¡œ ê³„ì† ì‹¤í–‰ë¨
- Dependency arrayì— í¬í•¨í•  í•„ìš” ì—†ìŒ!

---

### 3ï¸âƒ£ **CORS Errors on Multiple Vite Dev Ports** (Commit fa512cd9)

**ë¬¸ì œ**:
- Vite dev serverê°€ í¬íŠ¸ ì¶©ëŒ ì‹œ ìë™ìœ¼ë¡œ 5174, 5175, 5176ìœ¼ë¡œ fallback
- Backend CORSì—ì„œ 5173ë§Œ í—ˆìš© â†’ CORS ì—ëŸ¬

**í•´ê²°ì±…**:
```typescript
// backend/src/app.ts
const allowedOrigins = [
  'https://playful-cocada-a89755.netlify.app',
  'http://localhost:5173',
  'http://localhost:5174', // Added
  'http://localhost:5175', // Added
  'http://localhost:5176', // Added
  'http://localhost:3000',
];
```

---

### 4ï¸âƒ£ **Vertical Iris Tracking Sensitivity Too Low** (Commit accd1b02) âš ï¸

**ë¬¸ì œ**:
- ì‚¬ìš©ì í”¼ë“œë°±: "í™ì±„ì˜ ìƒí•˜ ì›€ì§ì„ì„ ë¯¼ê°í•˜ê²Œ ì¡ì•„ë‚´ì§€ ëª»í–ˆì–´"
- ìˆ˜ì§ ì‹œì„  ì´ë™ ì‹œ gaze markerê°€ ê±°ì˜ ì›€ì§ì´ì§€ ì•ŠìŒ

**ì›ì¸**:
```typescript
// ìˆ˜ì§ head pitch ì˜í–¥ë ¥ì´ ë„ˆë¬´ ë‚®ìŒ
const pitchInfluence = 0.05 * depthFactor; // TOO LOW!
```

**í•´ê²°ì±… (Commit accd1b02)**:
```typescript
// Increased from 0.05 to 8.0 - matches horizontal sensitivity
const enhancedPitchInfluence = 8.0 * depthFactor;
const depthCorrectedY = avgIrisRatioY + (headPitch * enhancedPitchInfluence);

// Added debug logging
if (import.meta.env.DEV && Math.random() < 0.033) {
  console.log('ğŸ“Š Vertical Tracking Components:', {
    avgIrisRatioY, headPitch, enhancedPitchInfluence,
    headContribution: (headPitch * enhancedPitchInfluence).toFixed(4),
    depthCorrectedY, avgZ, depthFactor
  });
}
```

**ìƒíƒœ**: í…ŒìŠ¤íŠ¸ ëŒ€ê¸° ì¤‘ (ë°°í¬ í›„ ì‚¬ìš©ì í”¼ë“œë°± í•„ìš”)

---

## ì§„í–‰ ì¤‘ì¸ ë¬¸ì œ (Current Issues)

### ğŸ”´ **Priority 1: Calibration Stage Video Stream Stability**

**í˜„ìƒ**:
- Calibration ë‹¨ê³„ ì§„ì… ì‹œ ì—¬ì „íˆ `readyState: 0` ë°œìƒ (ê°„í—ì )
- Face detectionì´ ì‘ë™í•˜ì§€ ì•ŠìŒ â†’ ì‹œì„  ë§ˆì»¤ í‘œì‹œ ì•ˆë¨

**ìµœê·¼ ì‹œë„í•œ í•´ê²°ì±…**:
- âœ… Commit 2a880658: `style` prop ê³ ì •
- âœ… Commit c255a577: Detection loop restart ë°©ì§€

**í˜„ì¬ ìƒíƒœ**: ë°°í¬ í›„ í…ŒìŠ¤íŠ¸ ëŒ€ê¸° ì¤‘

**ì¶”ê°€ ì¡°ì‚¬ í•„ìš” ì‚¬í•­**:
1. React Strict Modeê°€ useEffectë¥¼ ë‘ ë²ˆ ì‹¤í–‰í•˜ëŠ”ì§€ í™•ì¸
2. Netlify ë°°í¬ í™˜ê²½ì—ì„œ ë¸Œë¼ìš°ì € ì°¨ì´ í™•ì¸ (Chrome vs Safari)
3. Video elementì˜ `loadedmetadata` event listener ì¶”ê°€ ê³ ë ¤

---

### ğŸŸ¡ **Priority 2: Weak Vertical (Y-axis) Iris Tracking**

**í˜„ìƒ**:
- ìœ„/ì•„ë˜ ì‹œì„  ì´ë™ ì‹œ gaze markerì˜ Y ì¢Œí‘œ ë³€í™”ê°€ ì‘ìŒ
- Horizontal trackingì€ ì •ìƒ ì‘ë™

**ì›ì¸ ë¶„ì„**:
1. **Current Approach (2D Offset)**:
   ```typescript
   avgIrisRatioY = (leftIrisOffsetY + rightIrisOffsetY) / 2
   depthCorrectedY = avgIrisRatioY + (headPitch * enhancedPitchInfluence)
   ```
   - Iris Y offset ìì²´ê°€ ì‘ì€ ë²”ìœ„ (Â±0.03)
   - Head pitchë¡œ ë³´ì •í•˜ì§€ë§Œ ì—¬ì „íˆ ì œí•œì 

2. **JEOresearch/EyeTrackerì˜ ì ‘ê·¼ (3D Ray)**:
   ```python
   gaze_dir = (iris_3d - sphere_world) / norm(iris_3d - sphere_world)
   P = O + t * D  # Ray-plane intersection
   ```
   - 3D ray projection â†’ monitor plane intersection
   - ë” ì •í™•í•œ ìˆ˜ì§ íŠ¸ë˜í‚¹ ê°€ëŠ¥

**í•´ê²°ì±… í›„ë³´**:
- **Option A**: `enhancedPitchInfluence`ë¥¼ ë” ë†’ì„ (8.0 â†’ 12.0~15.0)
- **Option B**: 3D ray projection ë°©ì‹ìœ¼ë¡œ ì „ë©´ ë§ˆì´ê·¸ë ˆì´ì…˜ (ëŒ€ê·œëª¨ ë¦¬íŒ©í† ë§)

**í˜„ì¬ ìƒíƒœ**: Commit accd1b02 ë°°í¬ í›„ ì‚¬ìš©ì í”¼ë“œë°± ëŒ€ê¸°

---

### ğŸŸ¡ **Priority 3: Eye Center Bias from MediaPipe**

**ë¬¸ì œ (YouTube ì˜ìƒ & JEOresearch ì§€ì )**:
- MediaPipe eyelid landmarksëŠ” inherent bias ìˆìŒ
- ëˆˆì„ ìœ„ë¡œ ëœ¨ë©´ landmarkë„ ìœ„ë¡œ shift â†’ eye center ë¶€ì •í™•

**í˜„ì¬ êµ¬í˜„**:
```typescript
// Biased approach - using eyelid landmarks
const leftEyeX = (landmarks[33].x + landmarks[133].x) / 2;
const leftEyeY = (landmarks[159].y + landmarks[145].y) / 2;
```

**JEOresearch í•´ê²°ì±…**:
```python
# Stable approach - nose-based coordinate system
nose_scale = current_nose_distance / reference_nose_distance
sphere_world = nose_landmark + (local_eye_offset * nose_scale)
```

**ìƒíƒœ**: ê·¼ë³¸ ì›ì¸ì€ íŒŒì•…í–ˆìœ¼ë‚˜, ëŒ€ê·œëª¨ ë¦¬íŒ©í† ë§ í•„ìš” (Phase 2)

---

## í–¥í›„ ì‘ì—… ê³„íš (Roadmap)

### ğŸ¯ **Phase 1: Stability & Quick Wins** (ì§„í–‰ ì¤‘)

**ëª©í‘œ**: í˜„ì¬ ì‹œìŠ¤í…œ ì•ˆì •í™” ë° ê¸°ë³¸ calibration ì‘ë™

- [x] Video stream persistence ë¬¸ì œ í•´ê²° (Commits 6e4507f6, 2a880658, c255a577)
- [x] Vertical tracking sensitivity ê°œì„  ì‹œë„ (Commit accd1b02)
- [ ] **Calibration stage ì•ˆì •ì„± ê²€ì¦** (í…ŒìŠ¤íŠ¸ ëŒ€ê¸° ì¤‘)
- [ ] 9-point calibration ì™„ë£Œ workflow í…ŒìŠ¤íŠ¸
- [ ] Calibration data ì €ì¥ ë° ì¬ì‚¬ìš© ê²€ì¦

**ì˜ˆìƒ ì†Œìš” ê¸°ê°„**: 1-2ì£¼

---

### ğŸš€ **Phase 2: Advanced 3D Gaze Tracking** (ê³„íš ë‹¨ê³„)

**ëª©í‘œ**: JEOresearch/EyeTracker ë°©ì‹ì˜ 3D ray projection ë„ì…

#### 2.1 **Eye Sphere Stabilization**

**êµ¬í˜„ ì˜ˆì •**:
```typescript
interface EyeSphere3D {
  center: { x: number; y: number; z: number };
  radius: number;
}

const computeStableEyeSphere = (landmarks, videoWidth, videoHeight) => {
  // Nose landmarks (stable reference)
  const noseBridge = landmarks[168];
  const noseTip = landmarks[1];

  // Compute nose scale (depth awareness)
  const noseDistance = distance3D(noseTip, noseBridge);
  const noseScale = noseDistance / referenceNoseDistance;

  // Eye sphere centers (nose-based offset)
  const leftEyeSphere = {
    center: {
      x: noseBridge.x * videoWidth - (interOcularDistance / 2) * noseScale,
      y: noseBridge.y * videoHeight - eyeNoseVerticalOffset * noseScale,
      z: (noseBridge.z ?? 0) * videoWidth
    },
    radius: 0.012 * videoWidth * noseScale
  };

  return { left: leftEyeSphere, right: rightEyeSphere };
};
```

**ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ **: Eye center stability ~80% í–¥ìƒ

---

#### 2.2 **3D Gaze Ray Projection**

**êµ¬í˜„ ì˜ˆì •**:
```typescript
interface GazeRay3D {
  origin: { x: number; y: number; z: number };
  direction: { x: number; y: number; z: number };
}

const projectGazeRay = (eyeSphere: EyeSphere3D, irisCenter: Point3D) => {
  // Direction vector from eye sphere center to iris
  const direction = {
    x: irisCenter.x - eyeSphere.center.x,
    y: irisCenter.y - eyeSphere.center.y,
    z: (irisCenter.z ?? 0) - eyeSphere.center.z
  };

  // Normalize
  const length = Math.sqrt(direction.x**2 + direction.y**2 + direction.z**2);

  return {
    origin: eyeSphere.center,
    direction: {
      x: direction.x / length,
      y: direction.y / length,
      z: direction.z / length
    }
  };
};
```

---

#### 2.3 **Ray-Plane Intersection**

**êµ¬í˜„ ì˜ˆì •**:
```typescript
const computeGazeIntersection = (
  leftRay: GazeRay3D,
  rightRay: GazeRay3D,
  monitorPlane: { normal: Vector3D; distance: number }
) => {
  // Average gaze direction (combined ray)
  const combinedDirection = {
    x: (leftRay.direction.x + rightRay.direction.x) / 2,
    y: (leftRay.direction.y + rightRay.direction.y) / 2,
    z: (leftRay.direction.z + rightRay.direction.z) / 2
  };

  // Eye midpoint
  const eyeMidpoint = {
    x: (leftRay.origin.x + rightRay.origin.x) / 2,
    y: (leftRay.origin.y + rightRay.origin.y) / 2,
    z: (leftRay.origin.z + rightRay.origin.z) / 2
  };

  // Ray-plane intersection: P = O + t * D
  const dotOriginNormal = dot(eyeMidpoint, monitorPlane.normal);
  const dotDirNormal = dot(combinedDirection, monitorPlane.normal);
  const t = (monitorPlane.distance - dotOriginNormal) / dotDirNormal;

  // Intersection point
  const intersection = {
    x: eyeMidpoint.x + t * combinedDirection.x,
    y: eyeMidpoint.y + t * combinedDirection.y,
    z: eyeMidpoint.z + t * combinedDirection.z
  };

  return { x: intersection.x / videoWidth, y: intersection.y / videoHeight };
};
```

**ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ **: Vertical tracking ~150% í–¥ìƒ

---

#### 2.4 **Smoothing & Jitter Reduction**

**êµ¬í˜„ ì˜ˆì •**:
```typescript
// Deque-based smoothing (JEO ë°©ì‹)
const gazeHistory = useRef<Array<{ x: number; y: number }>>(
  Array(5).fill({ x: 0, y: 0 })
);

const smoothGaze = (rawGaze: { x: number; y: number }) => {
  gazeHistory.current.shift();
  gazeHistory.current.push(rawGaze);

  const sum = gazeHistory.current.reduce(
    (acc, val) => ({ x: acc.x + val.x, y: acc.y + val.y }),
    { x: 0, y: 0 }
  );

  return {
    x: sum.x / gazeHistory.current.length,
    y: sum.y / gazeHistory.current.length
  };
};
```

**ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ **: Jitter ~40% ê°ì†Œ

---

**Phase 2 ì˜ˆìƒ ì†Œìš” ê¸°ê°„**: 2-3ì£¼

**Phase 2 Decision Point**:
- Phase 1ì—ì„œ vertical trackingì´ ì¶©ë¶„íˆ ê°œì„ ë˜ë©´ Phase 2 ì—°ê¸° ê°€ëŠ¥
- ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ìš°ì„ ìˆœìœ„ ì¬ì¡°ì •

---

### ğŸŒŸ **Phase 3: Production Optimization** (ë¯¸ì •)

**ëª©í‘œ**: ì‹¤ì œ ì‚¬ìš©ì í™˜ê²½ì—ì„œì˜ ì„±ëŠ¥ ìµœì í™”

- [ ] Multi-browser compatibility (Chrome, Safari, Firefox, Edge)
- [ ] Mobile device support (iOS Safari, Android Chrome)
- [ ] Low-light environment handling
- [ ] Glasses/Sunglasses detection and compensation
- [ ] Performance profiling and optimization (60 FPS ìœ ì§€)
- [ ] A/B testing infrastructure

**ì˜ˆìƒ ì†Œìš” ê¸°ê°„**: 3-4ì£¼

---

## ê¸°ìˆ  ìŠ¤íƒ ìƒì„¸ (Tech Stack)

### **Frontend**

| ê¸°ìˆ  | ë²„ì „ | ìš©ë„ |
|------|------|------|
| **React** | 18.x | UI ì»´í¬ë„ŒíŠ¸ í”„ë ˆì„ì›Œí¬ |
| **TypeScript** | 5.x | íƒ€ì… ì•ˆì •ì„± |
| **Vite** | 5.x | ë¹Œë“œ ë„êµ¬ |
| **Tailwind CSS** | 3.x | ìŠ¤íƒ€ì¼ë§ |

### **Computer Vision**

| ê¸°ìˆ  | ë²„ì „ | ìš©ë„ |
|------|------|------|
| **MediaPipe Tasks Vision** | 0.10.22 | Face & Iris Landmark Detection |
| - FaceLandmarker | - | 478 facial landmarks |
| - Iris Landmarks | - | 5 landmarks per eye (468-477) |

### **Signal Processing**

| ì•Œê³ ë¦¬ì¦˜ | êµ¬í˜„ | ìš©ë„ |
|----------|------|------|
| **Kalman Filter** | Custom | Gaze coordinate noise reduction |
| **Polynomial Regression** | Custom | Calibration model (2nd order) |

### **Deployment**

| ì„œë¹„ìŠ¤ | ìš©ë„ |
|--------|------|
| **Netlify** | Frontend hosting (https://playful-cocada-a89755.netlify.app) |
| **Render** | Backend hosting |
| **GitHub** | Version control & CI/CD |

---

## í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ (Core Algorithms)

### 1. **Gaze Estimation (Current: 2D Offset Method)**

```typescript
// === HORIZONTAL (X-axis) ===
const leftIrisOffsetX = landmarks.leftIris.x - landmarks.leftEye.x;
const rightIrisOffsetX = landmarks.rightIris.x - landmarks.rightEye.x;
const avgIrisRatioX = (leftIrisOffsetX + rightIrisOffsetX) / (2 * videoWidth);

// Head yaw compensation
const eyesCenterX = (landmarks.leftEye.x + landmarks.rightEye.x) / 2;
const noseTipX = landmarks.noseTip.x;
const headYaw = (noseTipX - eyesCenterX) / videoWidth;

const baseSensitivityX = 35;
const headCompensatedX = (avgIrisRatioX * baseSensitivityX) - (headYaw * 8.0);

// === VERTICAL (Y-axis) ===
const leftIrisOffsetY = landmarks.leftIris.y - landmarks.leftEye.y;
const rightIrisOffsetY = landmarks.rightIris.y - landmarks.rightEye.y;
const avgIrisRatioY = (leftIrisOffsetY + rightIrisOffsetY) / (2 * videoHeight);

// Head pitch compensation
const eyesCenterY = (landmarks.leftEye.y + landmarks.rightEye.y) / 2;
const noseTipY = landmarks.noseTip.y;
const headPitch = -(noseTipY - eyesCenterY) / videoHeight;

// 3D depth factor
const leftZ = landmarks.leftIris.z ?? 0;
const rightZ = landmarks.rightIris.z ?? 0;
const avgZ = (leftZ + rightZ) / 2;
const depthFactor = Math.exp(-avgZ * 2.0);

const enhancedPitchInfluence = 8.0 * depthFactor; // NEW!
const depthCorrectedY = avgIrisRatioY + (headPitch * enhancedPitchInfluence);

// === FINAL GAZE COORDINATES ===
const rawGaze = {
  x: 0.5 + headCompensatedX,
  y: 0.5 - (depthCorrectedY * baseSensitivityX)
};
```

---

### 2. **Kalman Filter (Noise Reduction)**

**State Model**:
```typescript
// State vector: [x, y, vx, vy]
// Measurement: [x, y]

class KalmanFilter {
  // Process noise (system dynamics uncertainty)
  processNoise = 0.01;

  // Measurement noise (sensor uncertainty)
  measurementNoise = 0.1;

  predict() {
    // x_k = F * x_{k-1}
    // P_k = F * P_{k-1} * F^T + Q
  }

  update(measurement) {
    // K = P_k * H^T * (H * P_k * H^T + R)^{-1}
    // x_k = x_k + K * (z - H * x_k)
    // P_k = (I - K * H) * P_k
  }
}
```

**íš¨ê³¼**:
- Gaze coordinate jitter ~60% ê°ì†Œ
- 30 FPS â†’ smooth 60 FPS perceived motion

---

### 3. **Polynomial Calibration Model**

**Training**:
```typescript
// Collect N calibration points
const calibrationData = [
  { screen: {x: 0.1, y: 0.1}, gaze: {x: 0.15, y: 0.12} },
  { screen: {x: 0.5, y: 0.5}, gaze: {x: 0.48, y: 0.52} },
  // ... 9 points total
];

// Fit 2nd-order polynomial
// screenX = a0 + a1*gazeX + a2*gazeY + a3*gazeX^2 + a4*gazeY^2 + a5*gazeX*gazeY
const model = fitPolynomial(calibrationData, order = 2);
```

**Inference**:
```typescript
const calibratedGaze = applyCalibrationModel(rawGaze, model);
```

**ì •í™•ë„ ëª©í‘œ**: í‰ê·  ì˜¤ì°¨ < 2.0Â° (visual angle)

---

## ì°¸ê³  ìë£Œ (References)

### **YouTube Videos**

1. **"Webcam Eye Tracking in Python"**
   - URL: (ì‚¬ìš©ì ì œê³µ ì˜ìƒ)
   - í•µì‹¬ ë‚´ìš©:
     - MediaPipe ê¸°ë°˜ ì›¹ìº  ì‹œì„  ì¶”ì 
     - Eye center bias ë¬¸ì œ ì§€ì 
     - 3D ray projection ë°©ì‹ ì„¤ëª…

### **GitHub Repositories**

1. **JEOresearch/EyeTracker**
   - URL: https://github.com/JEOresearch/EyeTracker/tree/main/Webcam3DTracker
   - í•µì‹¬ íŒŒì¼: `MonitorTracking.py`
   - ì°¸ê³  ì•Œê³ ë¦¬ì¦˜:
     - Nose-based eye sphere stabilization
     - 3D gaze ray projection
     - Ray-plane intersection
     - Deque smoothing filter

### **MediaPipe Documentation**

1. **Face Landmark Detection**
   - URL: https://developers.google.com/mediapipe/solutions/vision/face_landmarker
   - 468 facial landmarks + 10 iris landmarks (5 per eye)
   - Subpixel accuracy

### **Academic Papers** (ê°„ì ‘ ì°¸ì¡°)

1. **Kalman Filtering for Gaze Tracking**
   - Process noise vs measurement noise tuning

2. **Polynomial Calibration Models**
   - 2nd order vs 3rd order trade-offs
   - Overfitting prevention

---

## Git Commit History (ì£¼ìš” ì»¤ë°‹)

| Commit | Date | Summary |
|--------|------|---------|
| `6e4507f6` | 2025-01 | Fix: Video stream persistence across calibration stages |
| `fa512cd9` | 2025-01 | Add CORS support for Vite dev server ports 5174-5176 |
| `accd1b02` | 2025-01 | Enhance vertical iris tracking sensitivity (0.05 â†’ 8.0) |
| `2a880658` | 2025-01 | Fix: Prevent video srcObject loss (constant style prop) |
| `c255a577` | 2025-01 | Fix: Prevent detection loop restart during stage transitions |

---

## ë°°í¬ ì •ë³´ (Deployment)

**Frontend (Netlify)**:
- URL: https://playful-cocada-a89755.netlify.app
- Branch: `main` (auto-deploy)
- Build: `npm run build` (Vite)

**Backend (Render)**:
- Endpoint: (ë°°í¬ëœ backend URL)
- Branch: `main` (auto-deploy)
- Build: `npm run build` (TypeScript)

**í™˜ê²½ ë³€ìˆ˜**:
```bash
# frontend/.env.local
VITE_API_URL=http://localhost:3000  # ë¡œì»¬ ê°œë°œ
# VITE_API_URL=<Render backend URL>  # í”„ë¡œë•ì…˜
```

---

## í…ŒìŠ¤íŠ¸ ê°€ì´ë“œ (Testing Guide)

### **ë¡œì»¬ ê°œë°œ í™˜ê²½**

```bash
# Backend
cd backend
npm install
npm run dev  # Port 3000

# Frontend
cd frontend
npm install
npm run dev  # Port 5173

# .env.local ì„¤ì • í™•ì¸
# VITE_API_URL=http://localhost:3000
```

### **Calibration í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤**

1. **Vision TEST** ì§„ì…
2. **Calibration ì‹œì‘** ë²„íŠ¼ í´ë¦­
3. **Camera Check**:
   - âœ… ì–¼êµ´ì´ ì¤‘ì•™ íƒ€ì› ê°€ì´ë“œ ì•ˆì— ìœ„ì¹˜
   - âœ… ì´ˆë¡ìƒ‰ í…Œë‘ë¦¬ + "ì™„ë²½í•©ë‹ˆë‹¤!" ë©”ì‹œì§€
   - âœ… 3ì´ˆ ì¹´ìš´íŠ¸ë‹¤ìš´
4. **Calibration Stage**:
   - âœ… 9ê°œ ì  ìˆœì°¨ì ìœ¼ë¡œ í‘œì‹œ
   - âœ… ê° ì ë‹¹ 3ì´ˆ ì‘ì‹œ
   - âœ… íŒŒë€ìƒ‰ ì‹œì„  ë§ˆì»¤ê°€ ì  ê·¼ì²˜ì— í‘œì‹œë˜ì–´ì•¼ í•¨
   - âœ… ì–¼êµ´ ì¸ì‹: ì´ˆë¡ìƒ‰ dot
5. **ì™„ë£Œ**:
   - âœ… "ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì™„ë£Œ!" ë©”ì‹œì§€
   - âœ… localStorageì— calibration data ì €ì¥ í™•ì¸

### **ë””ë²„ê¹… ì²´í¬ë¦¬ìŠ¤íŠ¸**

**Consoleì—ì„œ í™•ì¸í•  ë¡œê·¸**:
```
âœ… MediaPipe Tasks Vision initialized successfully
âœ… Camera stream obtained: {active: true, tracks: 2, videoTracks: 1}
âœ… Video playback started: {readyState: 4, videoWidth: 640, videoHeight: 480}
ğŸ”„ Starting detection loop
âœ… Face detected with 478 landmarks
ğŸ¯ Gaze updated: {x: 0.52, y: 0.48}
```

**ë¬¸ì œ ë°œìƒ ì‹œ í™•ì¸í•  ë¡œê·¸**:
```
â³ Video not ready: {readyState: 0, width: 0, height: 0}  â† Video stream ì†ì‹¤
âŒ Face detection failed  â† MediaPipe ì´ˆê¸°í™” ì‹¤íŒ¨
âš ï¸ Need at least 3 points, retrying calibration...  â† Gaze data ìˆ˜ì§‘ ì‹¤íŒ¨
```

---

## ì„±ëŠ¥ ëª©í‘œ (Performance Targets)

| ì§€í‘œ | ëª©í‘œ | í˜„ì¬ ìƒíƒœ |
|------|------|-----------|
| **Face Detection FPS** | 30 FPS | âœ… 30 FPS |
| **Gaze Update Latency** | < 50ms | âœ… ~33ms (30 FPS) |
| **Calibration Accuracy** | < 2.0Â° visual angle | âš ï¸ í…ŒìŠ¤íŠ¸ í•„ìš” |
| **Vertical Tracking Range** | Â±20Â° | âš ï¸ ê°œì„  ì¤‘ (accd1b02) |
| **Horizontal Tracking Range** | Â±30Â° | âœ… ì •ìƒ ì‘ë™ |
| **Jitter (std dev)** | < 0.5Â° | âœ… Kalman filter ì ìš© |
| **Browser Compatibility** | Chrome 90+, Safari 14+ | âš ï¸ Chromeë§Œ í…ŒìŠ¤íŠ¸ |

---

## ì•Œë ¤ì§„ ì œì•½ì‚¬í•­ (Known Limitations)

1. **MediaPipe Eye Center Bias**: Eyelid landmarks ì‚¬ìš© â†’ ëˆˆ ì›€ì§ì„ì— ë”°ë¼ center shift
2. **2D Gaze Estimation**: Depth information í™œìš© ë¶€ì¡± â†’ ìˆ˜ì§ íŠ¸ë˜í‚¹ ì œí•œì 
3. **Single Camera**: Stereo vision ì—†ìŒ â†’ ì •í™•í•œ 3D ìœ„ì¹˜ ì¶”ì • ì–´ë ¤ì›€
4. **Lighting Dependency**: ì €ì¡°ë„ í™˜ê²½ì—ì„œ landmark detection ì •í™•ë„ í•˜ë½
5. **Glasses/Contacts**: ë°˜ì‚¬ê´‘ìœ¼ë¡œ ì¸í•œ iris detection ê°„ì„­ ê°€ëŠ¥
6. **Head Movement**: ê³¼ë„í•œ ë¨¸ë¦¬ ì›€ì§ì„ ì‹œ calibration ì •í™•ë„ ì €í•˜

---

## ë‹¤ìŒ ë‹¨ê³„ (Next Steps)

### **ì¦‰ì‹œ ìˆ˜í–‰ (Immediate)**

1. âœ… Commit c255a577 ë°°í¬ í™•ì¸ (Netlify)
2. ğŸ”„ Calibration stage video stream ì•ˆì •ì„± í…ŒìŠ¤íŠ¸
3. ğŸ”„ Vertical tracking sensitivity ê°œì„  íš¨ê³¼ ê²€ì¦ (Commit accd1b02)

### **ë‹¨ê¸° (1-2ì£¼)**

1. Phase 1 ì™„ë£Œ: 9-point calibration ì•ˆì •í™”
2. ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ ë° ìš°ì„ ìˆœìœ„ ì¬ì¡°ì •
3. Phase 2 ì°©ìˆ˜ ì—¬ë¶€ ê²°ì •

### **ì¤‘ê¸° (1-2ê°œì›”)**

1. Phase 2 ì™„ë£Œ: 3D ray projection ë„ì… (ì¡°ê±´ë¶€)
2. Multi-browser í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸
3. Production readiness ê²€ì¦

---

## ë¬¸ì„œ ë²„ì „ ì •ë³´

- **ì‘ì„±ì¼**: 2025-01-20
- **ìµœì¢… ìˆ˜ì •**: 2025-01-20
- **ì‘ì„±ì**: Claude Code + ê°œë°œíŒ€
- **ë¬¸ì„œ ë²„ì „**: 1.0.0

**ë³€ê²½ ì´ë ¥**:
- 2025-01-20: ì´ˆê¸° ë¬¸ì„œ ìƒì„±, Phase 1 ì™„ë£Œ ì‹œì ê¹Œì§€ì˜ ë‚´ì—­ ì •ë¦¬
